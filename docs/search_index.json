[
["index.html", "R Textbook 1 Preface", " R Textbook Ian Cero’s CSP 518 Class Spring 2019 1 Preface What follows is an “organic” textbook. It is the product of class discussions and exercises, each of which was recorded as it was presented in class. "],
["basic-r.html", "2 Basic R 2.1 Writing in R and Rmarkdown 2.2 Variables 2.3 Vectors 2.4 Lists 2.5 Dataframes 2.6 Functions 2.7 Packages", " 2 Basic R 2.1 Writing in R and Rmarkdown 2.1.1 Chatting with R Using R is just a chat with the computer. “Hey, R. What is \\(1 + 2\\)?” 1 + 2 ## [1] 3 2.1.2 Rmarkdown tricks To make text bold, we add two **s around it. To make text italicized, we add just one * around it. If we need special characters (like * or $), then we just add a forward “\\” in front of them (but not behind). Math symbols in your text are process with Latex, just put an “$” before and after your math. Like this, $y = x$ becomes \\(y = x\\). 2.1.3 Code blocks To make a code block, press CTRL+ALT+I. banana &lt;- 5 banana + 1 ## [1] 6 2.2 Variables Variables are values that I want to give names to and save for later. 2.2.1 The assignment operator We make variables with the &lt;- operator. This is called the assignment operator because it assigns values on the right to names on the left. If I want to know what the value of a variable is, I can run it alone on its own line. my_special_var &lt;- 1 + 2 my_special_var ## [1] 3 You can TECHNICALLY use = for assignment too. Never do this. my_other_var = 12 my_other_var + my_special_var ## [1] 15 The = symbol gets also used for a few other things in R. So, using it to assign variables will make your code more confusing to you, when you go back to read it over later. 2.2.2 Numerics Doubles Doubles are decimal numbers, like \\(1.1, 2.2, 3.0\\). If I make a number variable without doing anything special, R defaults to a double. a &lt;- 1.1 b &lt;- 2.0 is.double(a) ## [1] TRUE is.double(b) ## [1] TRUE Integers Integers must have an L after them. That is how R knows that you don’t want a double, but instead want a “long-capable integer”. c &lt;- 1L d &lt;- 1 is.integer(c) ## [1] TRUE is.integer(d) ## [1] FALSE Here is a useful cheatsheet for the different numeric operators and how they behave. Operator Expression Result + 10 + 3 13 - 10 - 3 7 * 10 * 3 30 / 10 / 3 3.333 ^ 10 ^ 3 1000 %/% 10 %/% 3 3 %% 10 %% 3 1 Why care about the difference? Almost 99% of the time, this wont matter. But, with big data, integers take up must less memory. my_integers &lt;- seq(from = 1L, to = 1e6L, by = 1L) my_doubles &lt;- seq(from = 1.0, to = 1e6, by = 1.0) object.size(my_integers) ## 4000048 bytes object.size(my_doubles) ## 8000048 bytes Note here that although we are using only whole numbers from 1 to 1 million, the first sequence (my_integers) is stored as an integer and the second sequence (my_doubles) is stored as a number that may include decimals. This second case needs more space (twice as much) to be allocated in advance, even if we never use those decimal places. Again, this will almost never matter for most people, most of the time. However, it is good to be aware of for when your datasets get large (i.e., several million cases or more). 2.2.3 Characters Characters are text symbols and they are made with either &quot;&quot; or '', either works. a &lt;- &#39;here is someone\\&#39;s text&#39; b &lt;- &quot;here is more text&quot; a ## [1] &quot;here is someone&#39;s text&quot; b ## [1] &quot;here is more text&quot; To combine two strings, I use paste(). paste(a, b) ## [1] &quot;here is someone&#39;s text here is more text&quot; If I dont want a space, then I used paste0(). paste0(a, b) ## [1] &quot;here is someone&#39;s texthere is more text&quot; 2.2.4 Booleans These are True and False values. You make them with the symbols T or TRUE and F or FALSE. x &lt;- T y &lt;- F To compare them, we can use three operators. &amp; is “and” | is “or” ! is “not” (just give me the opposite of whatever is after me) x &amp; y # false ## [1] FALSE x | y # true ## [1] TRUE x &amp; !y # true ## [1] TRUE We can also have nested equations z &lt;- F x &amp; !(y | z) # true ## [1] TRUE We can also compare numbers. a &lt;- 1 b &lt;- 2 a &lt; 1 ## [1] FALSE a &lt;= 1 ## [1] TRUE a == 1 ## [1] TRUE If I want to compare multiple numbers, I need to do it seperately. (a &gt; 1) | (b &gt; 1) ## [1] TRUE Remember that booleans are ultimately numeric values underneath. d &lt;- T k &lt;- F u &lt;- 5 d*u ## [1] 5 d*k ## [1] 0 as.numeric(d) ## [1] 1 as.numeric(k) ## [1] 0 2.2.5 Special types NA - missing is.na(NA) ## [1] TRUE NaN - you did math wrong 0/0 ## [1] NaN Inf - infinity -5/0 ## [1] -Inf 2.3 Vectors R is built is on vectors. Vectors are collections of a bunch of values of the same type. my_vec &lt;- c(1, 5, 3, 7) my_vec ## [1] 1 5 3 7 If I try to put different types together, they go to the most primitive type (usually a character string). my_other_vec &lt;- c(22, &#39;orange&#39;, T) my_other_vec ## [1] &quot;22&quot; &quot;orange&quot; &quot;TRUE&quot; my_third_vec &lt;- c(T, F, 35) my_third_vec ## [1] 1 0 35 We can also missing values. my_fourth_vec &lt;- c(1, 4, 5, NA) my_fourth_vec ## [1] 1 4 5 NA is.na(my_fourth_vec) ## [1] FALSE FALSE FALSE TRUE If I want to combine two vectors… a &lt;- c(1, 2, 3) b &lt;- c(3, 5, 7) c(a, b) ## [1] 1 2 3 3 5 7 A brief example of matrices matrix( data = c(a, b), nrow = 2, byrow = T) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 3 5 7 Sometimes I want special vectors, direct sequences of numbers. There are two ways to do this. If all I want is a integer sequence (made of doubles), then I use the “&lt;first number&gt;:&lt;last number&gt;”. 1:5 ## [1] 1 2 3 4 5 5:1 ## [1] 5 4 3 2 1 Other times, I need to count by something other than one, so I use seq(from = &lt;start&gt;, to = &lt;end&gt;, by = &lt;number to count by&gt;) seq(from = 1, to = 7, by = 1.3) ## [1] 1.0 2.3 3.6 4.9 6.2 Hint: for brevity, I can leave off function parameter names, as long as I enter them in order seq(1, 7, by = 1.3) ## [1] 1.0 2.3 3.6 4.9 6.2 If I add a constant to a vector, then they all go up by that constant. 1:5 / 3 ## [1] 0.3333333 0.6666667 1.0000000 1.3333333 1.6666667 I can do math with equal-length sequences too. 1:5 - seq(1, 4, by = .7) ## [1] 0.0 0.3 0.6 0.9 1.2 But they must be equal lengths. 1:5 / 1:4 ## Warning in 1:5/1:4: longer object length is not a multiple of shorter ## object length ## [1] 1 1 1 1 5 To access the elements of a vector, I put a number OR booleans in brackets []. my_vec &lt;- c(&#39;apple&#39;, &#39;orange&#39;, &#39;banana&#39;, &#39;pair&#39;) my_vec[2] ## [1] &quot;orange&quot; my_vec[2:4] ## [1] &quot;orange&quot; &quot;banana&quot; &quot;pair&quot; my_vec[c(3, 2, 1, 4)] ## [1] &quot;banana&quot; &quot;orange&quot; &quot;apple&quot; &quot;pair&quot; I can also use bools. my_other_vec &lt;- c(1, 4, 6, 7, 9, 3, 9) my_other_vec &lt; 5 ## [1] TRUE TRUE FALSE FALSE FALSE TRUE FALSE my_other_vec[my_other_vec &lt; 5] ## [1] 1 4 3 I can also use functions that return values to access vectors, if I am creative… my_other_vec[max(my_other_vec) == my_other_vec] ## [1] 9 9 R also has special vectors that are pre-loaded. The most commonly used are letters and LETTERS, which return the lower-case letters and uppercase letters of the English alphabet, respectively. vec &lt;- c(1, 3, 4, 5, 3, 2, NA) mean(vec, na.rm = T) ## [1] 3 2.4 Lists &lt;&lt; More on lists to come &gt;&gt; Lists are special vectors that can hold multiple types of elements, even vectors my_vec &lt;- c(4, 5, 6) my_list &lt;- list(1, &#39;banana&#39;, 3, NA, my_vec) my_list ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;banana&quot; ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] NA ## ## [[5]] ## [1] 4 5 6 2.5 Dataframes 2.5.1 Construction Dataframes are spreadsheets. Under the hood of R, they are just lists of vectors, where all the vectors are required to be the same length. To make one, you can call the data.frame() function and put your vectors inside. heights &lt;- c(60, 65, 71, 72, 64) sexes &lt;- c(&#39;female&#39;, &#39;female&#39;, &#39;male&#39;, &#39;male&#39;, &#39;female&#39;) shoes &lt;- c(&#39;Adidas&#39;, &#39;Nike&#39;, &#39;Nike&#39;, &#39;Salvatore Ferragamo&#39;, &#39;Reebok&#39;) df &lt;- data.frame(height = heights, sex = sexes, shoes = shoes) df ## height sex shoes ## 1 60 female Adidas ## 2 65 female Nike ## 3 71 male Nike ## 4 72 male Salvatore Ferragamo ## 5 64 female Reebok 2.5.2 Built-in dataframes R has numerous built-in datasets that are ideal for demonstration purposes. We can get access to them using the data() command. This will load the data into our session, so we can then look at it. data(&#39;mtcars&#39;) mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Some datasets do not come in the form of a dataframe right away, but they can be converted into one using the as.data.frame() function. data(Seatbelts) is.data.frame(Seatbelts) ## [1] FALSE seatbelts_df &lt;- as.data.frame(Seatbelts) is.data.frame(seatbelts_df) ## [1] TRUE 2.6 Functions A function is a piece of code that does work for you. It takes inputs and (usually) returns outputs. For example, the sum() function takes the sum of a numeric vector. my_vec &lt;- c(3, 6, 2, 3) sum(my_vec) ## [1] 14 2.6.1 Getting help If I ever need to know something about a function, I can put a question mark in front of it (no ()s) and run that line. That will bring up the help document for that function. ?sum 2.6.2 Function parameters In addition to the data they take as input, most functions have additional parameters (sometimes called “arguments”, but they mean the same thing). Looking at its help file, the sum() function has two parameters: ..., the numbers you want to sum na.rm = FALSE, which tells sum() whether you want to remove (‘rm’) missing values (‘na’) before summing. Let’s look at what happens when we try to sum() a vector with a missing value. my_vec &lt;- c(5, NA, 2, 3) # should be 10 sum(my_vec) ## [1] NA R tells us the answer is missing (NA) because at least one of the vector elements is missing. This is to be conservative and to force you never to ignore missing values by accident. But what do we do if we really do want to sum all available values, ignoring the missing values. Again, looking at the help file, we can see that the na.rm parameter of the function is followed by = FALSE, under the Usage heading of that help document (look for sum(..., na.rm = FALSE)). This tells us that the parameter na.rm, which tells sum() whether to remove missing values from the calulation, defaults to FALSE. To get sum() to ignore the missing values in our vector, we simply set na.rm to TRUE (or T for short). sum(my_vec, na.rm = T) # should be 10 ## [1] 10 2.7 Packages Packages are collections of functions that someone else put together for you. You can install them using the install.packages() function, with the name of your package inside the () - don’t forget to use either single (' ') or double quotes (&quot; &quot;) around the package name too. install.packages(&#39;ggplot2&#39;) Once installed, use the library() function to load your package into your R session. Note, you don’t need quotes here. library(ggplot2) "],
["visualization.html", "3 Visualization 3.1 Base R 3.2 ggplot", " 3 Visualization There are many ways to visualize data in R. Two of the most common include Base R’s built-in functions and the ggplot2 package. 3.1 Base R The term “Base R” refers to the set of packages and functions that R loads into each session by default. These packages include several statistical and plotting functions you will probably use a lot. Some of the most common plotting functions are given below. hist() rpois(100, lambda = 3) ## [1] 2 1 1 3 4 1 6 2 5 3 3 2 0 2 3 2 4 3 3 5 8 3 3 3 3 3 3 2 4 5 1 2 4 4 1 ## [36] 4 2 7 4 1 4 1 3 6 2 3 3 1 4 6 5 3 1 1 6 3 4 3 0 3 3 1 1 3 1 3 6 3 4 0 ## [71] 2 7 3 2 2 3 4 4 4 7 1 5 1 2 3 2 0 4 1 3 2 1 5 4 1 2 1 4 1 2 my_vals &lt;- rexp(100, 10) hist( my_vals, breaks = 20, main = &#39;Plot of Exponential Dis.&#39;, xlab = &#39;X Values&#39;, ylab = &#39;Y values&#39;, col = &#39;cyan&#39;) barplot() heights &lt;- 1:10 barplot(heights) plot() x_vals &lt;- 1:10 y_values &lt;- rnorm(10) plot(x_vals, y_values, main = &#39;Banana&#39;, xlab = &#39;My X&#39;) 3.2 ggplot All statistical graphics share the same deep structure, a “Grammar of Graphics”. This means that any given plot can be uniquely identified (reproduced), given the following inputs. A dataset A coordinate system (2d Cartesian plane, 3d spherical coordinates, even quaternions!1) A facet specification (panel arrangement) Mappings from variables to aesthetic features One scale for each aesthetic mapping One or more layers of geometic objects (“geoms”), statistical functions, and position adjustments The most popular graphics package in R to date (maybe even accross all programming languages) is ggplot2. An update of the original ggplot package, ggplot2 is based on the Grammar of Graphics above. It functions are designed specifically to behave like a grammar, in which you add plot elements together like a sentence. Lets explore how to build a plot. We’ll start by loading the ggplot2 package and one of its built-in datasets (called mpg). This dataset includes information on 234 cars that will be convenient for us to plot. install.packages(&#39;ggplot2&#39;) library(ggplot2) data(mpg) mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 audi a4 q… 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 q… 1.8 1999 4 auto… 4 16 25 p comp… ## 10 audi a4 q… 2 2008 4 manu… 4 20 28 p comp… ## # … with 224 more rows 3.2.1 Blank ggplot plot ggplot(mpg) 3.2.2 Geoms Getting points to display (displ, hwy) ggplot(mpg) + geom_point(mapping = aes(x = displ, y = cty)) ggplot(mpg, mapping = aes(x = displ)) + geom_point(mapping = aes(y = cty), color = &#39;steelblue&#39;) + geom_point(mapping = aes(y = hwy), color = &#39;tomato&#39;) Other features of points we can change (size = cyl, color = class) ggplot(mpg, mapping = aes(x = displ, shape = drv, color = drv)) + geom_point(mapping = aes(y = cty)) + geom_point(mapping = aes(y = hwy)) Smoothed line —— (2nd block) Lines data(beavers) ggplot(beaver1, mapping = aes(x = time, y = temp)) + geom_point() + geom_line() Vlines and Hlines Density plots Histograms Boxplots 3.2.3 Stat Functions 3.2.4 Themes 3.2.5 Labels 3.2.6 Scales 3.2.7 Saving your plot** File type Quality Dimensions 3.2.8 Facets &lt;&gt; 3.2.9 Where to learn more The ggplot cheatsheet is available here as is the quickest way to look up something you might have briefly forgotten. For more complex questions, https://ggplot2.tidyverse.org/index.html contains numerous worked examples that will bring your plots from plain old publishable to down right beautiful. If you have a really specific question, the “ggplot” tag on Stack Overflow is your best friend. Ask and you shall receive (help)! Quaternions are… hard to describe. They are basically 4-part complex numbers that satisfy certain conditions. This allows them to elegently represent space in higher than 3-dimensions. However, they have been despised for much of their history. In fact, William Thomson, first baron Kelvin, note in 1892 “quaternions..though beautifully ingenious, have been an unmixed evil to those who have touched them in any way…”↩ "],
["functions-1.html", "4 Functions 4.1 Basic Example 4.2 Anatomy of an R function 4.3 Base R functions 4.4 Pipes 4.5 More talk about the help files 4.6 Functions from other packages 4.7 ", " 4 Functions Functions allow you to automate boring tasks so that you don’t have to keep doing the same thing over and over again. There are several advantages of this. As stated above, it is faster for you. When something changes (e.g., a reviewer wants you to exclude outliers), you only need to change your code in one place, rather than several. You eliminate the risk of “copy-and-paste” mistakes. This chapter covers the basics of function construction and use. 4.1 Basic Example x &lt;- 1:10 y &lt;- 2:20 z &lt;- 3:34 # Obscure reviewer request x &lt;- x/2 x &lt;- sqrt(x) x &lt;- round(x) y &lt;- y/2 y &lt;- sqrt(y) y &lt;- round(x) z &lt;- z/2 z &lt;- sqrt(z) z &lt;- round(z) 4.2 Anatomy of an R function Making functions speeds up the process and makes it safer. To make a function, we need to use the function() command, assign it to a name, provide potential parameters, then give it code to execute. reviewer_transformation &lt;- function(vals) { vals &lt;- vals/2 vals &lt;- sqrt(vals) vals &lt;- round(vals) vals } # Obscure reviewer request x &lt;- 1:10 x &lt;- x/2 x &lt;- sqrt(x) x &lt;- round(x) x ## [1] 1 1 1 1 2 2 2 2 2 2 other_x &lt;- 1:10 transformed_x &lt;- reviewer_transformation(other_x) x ## [1] 1 1 1 1 2 2 2 2 2 2 Name Parameters/Arguments Defaults Code Return values \\[ r = \\frac{\\text{cov}(x, y)}{\\sigma_x\\sigma_y} \\] # set.seed(314159) x &lt;- 1:10 y &lt;- x*rpois(n = 10, lambda = 3) w &lt;- 1:20 u &lt;- runif(n = 20) cor(x, y) ## [1] 0.7871337 cov(x, y)/(sd(x)*sd(y)) ## [1] 0.7871337 print(&#39;---&#39;) ## [1] &quot;---&quot; correlation &lt;- function(a, b) { cov(a, b)/(sd(a)*sd(b)) } cor(w, u) ## [1] -0.04037588 correlation(w, u) ## [1] -0.04037588 4.3 Base R functions Basic stat functions Random numbers! x &lt;- rpois(100, 10) y &lt;- 2*x + rnorm(100) df &lt;- data.frame(x, y) library(ggplot2) ggplot(df, aes(x, y)) + geom_point() + geom_hline(yintercept = mean(y), color = &#39;red&#39;) Setting the random seed paste() and paste0() 4.3.1 Scoping What happens inside a function, stays inside a function Discussion of shadow variables 4.3.2 Conditional execution set.seed(314159) x &lt;- 1:10 y &lt;- x*rpois(n = 10, lambda = 3) x[4] &lt;- 1 y[5] &lt;- 2 correlation &lt;- function(a, b) { if(any(is.na(a)) &amp; !any(is.na(b))){ return(&#39;Only one missing data issue.&#39;) } else if(any(is.na(a)) &amp; any(is.na(b))){ return(&#39;You have missing data butthead!&#39;) } else { cov(a, b)/(sd(a)*sd(b)) } } cor(x, y) ## [1] 0.8438 correlation(a = x, b = y) ## [1] 0.8438 if() if()/else() else if() ifelse() 4.3.3 the return() command ————— Part II —————– 4.4 Pipes Sometimes (often) it is useful to execute several functions in a row, using the answer from the last function as the input for the next function. But this can make your code redundant and a little unclear. reviewer_transformation &lt;- function(vals) { vals &lt;- vals/2 vals &lt;- sqrt(vals) vals &lt;- round(vals) vals } reviewer_transformation2 &lt;- function(vals) { round(sqrt(vals/2)) } reviewer_transformation_pipe &lt;- function(vals) { vals %&gt;% magrittr::divide_by(2) %&gt;% sqrt() %&gt;% round() } x &lt;- 1:10 reviewer_transformation(x) ## [1] 1 1 1 1 2 2 2 2 2 2 reviewer_transformation2(x) ## [1] 1 1 1 1 2 2 2 2 2 2 reviewer_transformation_pipe(x) ## [1] 1 1 1 1 2 2 2 2 2 2 library(magrittr) ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract fun1 &lt;- function(x){ x *2 } fun2 &lt;- function(y) { y - 3 } fun2(fun1(x)) ## [1] -1 1 3 5 7 9 11 13 15 17 x %&gt;% fun1() %&gt;% fun2() ## [1] -1 1 3 5 7 9 11 13 15 17 To address this, we use pipes. 4.4.1 Basic usage The %&gt;% operator (pronounced “pipe”) takes whatever is on its left-hand-side (LHS) and applies it to the first parameter of the function listed on the right-hand-side. Install and load the magrittr package to use it. install.packages(&#39;magrittr&#39;) library(magrittr) 4.4.2 Sending the LHS to another parameter But what do I do when I need the LHS output to go to another parameter in the RHS function? I use the . to remind the RHS function I want the LHS output to go to a different place. 4.4.3 Convenient magrittr functions 4.5 More talk about the help files 4.6 Functions from other packages I can import functions from another package by… What if I only want to import once? Namespace considerations 4.7 "],
["dataframes-1.html", "5 Dataframes 5.1 Storing rectangular data 5.2 Accessing data from a dataframe 5.3 Manupilating data 5.4 Using dplyr pipelines 5.5 dplyr pipelines with the magrittr %&gt;%", " 5 Dataframes In this chapter, we will address four issues: How to store rectanglar data How to access the data we have stored How to modify an existing dataset Using dplyr and magrittr pipes to make these tasks much easier (Praise Hadley Wickam!) 5.1 Storing rectangular data Rectangular or “relational” data are the kind with which you are undoubtedly most familiar. In rectangular datasets, rows represent observations (e.g., people, days, soil samples) and columns represent attributes of those observations (e.g., age, temperature, acidity). Because rectangular datasets are so common and so flexible, R has a special framework for storing and manipulating them: the dataframe. There are really only two rules: Columns can have different kinds of data (e.g., strings, numbers), but within a column everything has to be of the same type. All columns have to be of exactly equal length. If I have 10 values (including NA) in my first column, I need exactly 10 in all the other columns. 5.1.1 Making dataframes You can make a dataframe with the data.frame() command, just feed it some data and assign it to a variable. gender_data &lt;- c(&#39;Male&#39;, &#39;Female&#39;, &#39;Female&#39;, &#39;Male&#39;, &#39;Female&#39;) shoe_data &lt;- c(&#39;Nikes&#39;, &#39;Reebok&#39;, &#39;Adididas&#39;, &#39;Manolos&#39;, &#39;Nikes&#39;) age_data &lt;- c(18, NA, 19, 21, 18) df &lt;- data.frame(gender_data, shoe_data, age_data) df ## gender_data shoe_data age_data ## 1 Male Nikes 18 ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 4 Male Manolos 21 ## 5 Female Nikes 18 If I want to give specific names to my columns, I can do that too. df &lt;- data.frame(gender = gender_data, shoe = shoe_data, age = age_data) df ## gender shoe age ## 1 Male Nikes 18 ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 4 Male Manolos 21 ## 5 Female Nikes 18 5.1.2 Strings as factors In chapter 1, we talked about different kinds of data, including numbers, strings, and booleans. There are a few other kinds too, like factors. These are strings that are being used to represent categorical data (which may or may not be ordinal). For historical reasons, R’s data.frame() function defaults to turning any string variable into a factor. Most of the time, this doesn’t matter. However, if you ever need your strings to stay strings, then simply set the stringsAsFactors argument to false when you are making your dataframe. df &lt;- data.frame( gender = gender_data, shoe = shoe_data, age = age_data, stringsAsFactors = FALSE) df ## gender shoe age ## 1 Male Nikes 18 ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 4 Male Manolos 21 ## 5 Female Nikes 18 5.2 Accessing data from a dataframe 5.2.1 Basic framework I can access specific rows and columns using the df[&lt;rows&gt;, &lt;columns&gt;] framework. For example, if I want the 4th row and 3rd column of the dataframe above, I can enter: df[4, 3] ## [1] 21 If I leave an entry before or after my comma (,) empty, then R will give me all the rows or all the columns, respectively. df[4, ] # give me row 4, with all the columns ## gender shoe age ## 4 Male Manolos 21 df[, 3] # give me column 3, with all the rows ## [1] 18 NA 19 21 18 Sometimes, I might want more than one row. In that case, I just enter a vector of values I want. Note, I can even ask for them out of order (and they will be delivered out of order). df[c(3, 1, 2), 3] ## [1] 19 18 NA The same rules apply for requesting multiple columns. I can ask for a vector of columns (in whatever order I want - notice the code below will rearrange them, compared to the first code block above). df[c(3, 1, 2), c(2, 3, 1)] ## shoe age gender ## 3 Adididas 19 Female ## 1 Nikes 18 Male ## 2 Reebok NA Female 5.2.2 Using string column names I can also request columns by their “string” names (e.g., ‘shoe’, ‘age’). df[c(3, 1, 2), c(&#39;shoe&#39;, &#39;age&#39;)] ## shoe age ## 3 Adididas 19 ## 1 Nikes 18 ## 2 Reebok NA 5.2.3 Using the $ operator I can also use the $ operator to request a specific column. df$gender ## [1] &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; 5.2.4 Using booleans Note that I can also ask for rows (or columns) with T/F (boolean) values. You can think of this as R asking, “tell me true or false for which rows you want.” For example, if I wanted to analyze the female members of the dataset only, I would put T values for each of their rows, like so. is_female &lt;- c(F, T, T, F, T) df[is_female, ] ## gender shoe age ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 5 Female Nikes 18 This might look pretty tedious - and it is. However, this same system can be greatly sped up by using other dataframe tricks too, like the $ operator. is_female &lt;- df$gender == &#39;Female&#39; df[is_female, ] ## gender shoe age ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 5 Female Nikes 18 If we wanted to make this even shorter, we could simply write this single-line version below. You can read this in English as “give me my dataframe, but with only the rows where the gender is equal to female”. df[df$gender == &#39;Female&#39;, ] ## gender shoe age ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 5 Female Nikes 18 5.3 Manupilating data Much of what we want to do in R is manipulate data that are net yet in a form where they can be analyzed. We can do that by simply assigning values to the dataframe, using the same access convensions (numbered columns, named columns, or the $ operator) above. Lets make a column that tells us how long the participants have been legally an adult (i.e., how many years since they turned 18). df[, 4] &lt;- df$age - 18 df ## gender shoe age V4 ## 1 Male Nikes 18 0 ## 2 Female Reebok NA NA ## 3 Female Adididas 19 1 ## 4 Male Manolos 21 3 ## 5 Female Nikes 18 0 This worked, but it failed to give us a convenient name for our variable because we didn’t give R any way to name our variables. Let’s try using the named column approach. df[, &#39;years_as_adult&#39;] &lt;- df$age - 18 df ## gender shoe age V4 years_as_adult ## 1 Male Nikes 18 0 0 ## 2 Female Reebok NA NA NA ## 3 Female Adididas 19 1 1 ## 4 Male Manolos 21 3 3 ## 5 Female Nikes 18 0 0 That gave us exactly what we needed, a new named column. But we still have that pesky old column that go misnamed. To delete a dataframe column, assign it the value of NULL. df[, 4] &lt;- NULL df ## gender shoe age years_as_adult ## 1 Male Nikes 18 0 ## 2 Female Reebok NA NA ## 3 Female Adididas 19 1 ## 4 Male Manolos 21 3 ## 5 Female Nikes 18 0 That’s better. Now lets try one other method for manipulating our data, using the $ operator. Let’s make a new column indicating how long it will be until the participants in our dataset are of the legal age to consume alcohol in the United States. df$years_unti_alc &lt;- 21 - df$age 5.4 Using dplyr pipelines Our Lord and Savior, Hadley Wickham has given us the gift of dplyr in 2014. If you’re new to R and trying to understand how important that is, it was a lot like when Promethius gave humanity the gift of fire, only he wasn’t chained to a rock for doing so - that I know of. The general motivation for the dplyr package is that there are only a small number of ways you should be working with rectangular data to achieve 99% of the things you care about. That package makes these very easy to do. The main functions provided by dplyr are: select() - for selecting columns mutate() - for changing calumns filter() - for selecting rows group_by() - for performing tasks within groups summarize() - for aggregating data (e.g., group means, variances) The dplyr position is generally: if you want to do something other than one of these five things, you are probably behaving irresponsibly. Often this is true, the dplyr framework really has captured the general principles of working with rectangular data well. On the rare instance when you reeeeally need something else, you’ll have to code it yourself with the tools from the first section of this chapter or a custom function with the tools from the previous chapter. 5.4.1 How do these functions work? dplyr functions all work by taking at least two inputs: (1) a dataframe and (2) what you want to do to that dataframe. They all return the same thing: another dataframe, with the modifications you requested. 5.4.1.1 select() For example, if I use the select() function and list the columns I want, I will get a new dataframe with just those columns. Notice I don’t need to put quotes in dplyr functions. There are some complicated reasons for this that involve black magic from the Tidyverse, which I’ll tell you about when you’re older. library(dplyr) select(df, shoe) ## shoe ## 1 Nikes ## 2 Reebok ## 3 Adididas ## 4 Manolos ## 5 Nikes I can also just keep asking for more columns, by using more commas. select(df, shoe, age) ## shoe age ## 1 Nikes 18 ## 2 Reebok NA ## 3 Adididas 19 ## 4 Manolos 21 ## 5 Nikes 18 Most often, is a good idea to save my modified dataframe to another dataframe. modified_df &lt;- select(df, shoe, age) modified_df ## shoe age ## 1 Nikes 18 ## 2 Reebok NA ## 3 Adididas 19 ## 4 Manolos 21 ## 5 Nikes 18 5.4.1.2 mutate() To make new columns, I can use the mutate() function and specify what my new columns should be equal to. df &lt;- mutate(df, age_squared = age^2) df ## gender shoe age years_as_adult years_unti_alc age_squared ## 1 Male Nikes 18 0 3 324 ## 2 Female Reebok NA NA NA NA ## 3 Female Adididas 19 1 2 361 ## 4 Male Manolos 21 3 0 441 ## 5 Female Nikes 18 0 3 324 5.4.1.3 filter() To eliminate anyone who doesn’t meet some condition. Use the filter command and give it a condition that results in either TRUE or FALSE. under_19_df &lt;- filter(df, age &lt; 19) under_19_df ## gender shoe age years_as_adult years_unti_alc age_squared ## 1 Male Nikes 18 0 3 324 ## 2 Female Nikes 18 0 3 324 5.4.2 group_by() This function is best used in combination with other dplyr functions, as it doesn’t do much on it’s own. Technically, it creates a “grouped tibble”, which is special kind of dataframe that knows you want to do all your dplyr operations within each group. Notice, however, you can’t see that anything has changed. group_by(df, shoe) ## # A tibble: 5 x 6 ## # Groups: shoe [4] ## gender shoe age years_as_adult years_unti_alc age_squared ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Male Nikes 18 0 3 324 ## 2 Female Reebok NA NA NA NA ## 3 Female Adididas 19 1 2 361 ## 4 Male Manolos 21 3 0 441 ## 5 Female Nikes 18 0 3 324 But if we use group_by() is the input for a mutate() command, we can compute the mean age of each gender (see last column of the newly created dataframe). df &lt;- mutate(group_by(df, gender), mean_age = mean(age, na.rm = T)) 5.4.2.1 summarize() This summarizes some feature of your dataframe by applying an aggregate function to a given column (e.g., mean, max, variance). summarise(df, age_mean = mean(age, na.rm = T)) ## # A tibble: 2 x 2 ## gender age_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 18.5 ## 2 Male 19.5 Most often, it is used in combination with group_by(). summarise(group_by(df, gender), age_mean = mean(age, na.rm = T)) ## # A tibble: 2 x 2 ## gender age_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 18.5 ## 2 Male 19.5 5.5 dplyr pipelines with the magrittr %&gt;% All dplyr functions are designed to be used in combination with the magrittr pipe. This makes using several of them in a row easy to write and easy to read. Imagine we start with the same dataframe at the beginning of the chapter. Let’s use a pipeline to filter out any missing age values, assign new variable, years_as_adult, and explore the mean and standard deviation of that new variable. First, we reset that dataframe df &lt;- data.frame( gender = gender_data, shoe = shoe_data, age = age_data, stringsAsFactors = FALSE) df ## gender shoe age ## 1 Male Nikes 18 ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 4 Male Manolos 21 ## 5 Female Nikes 18 Then we make a pipeline, performing all of the tasks we just described. After each task, we make sure to use a pipe, so that our result gets fed into the next function. df %&gt;% filter(!is.na(age)) %&gt;% mutate(years_as_adult = age - 18) %&gt;% group_by(gender) %&gt;% summarise( mean = mean(years_as_adult), sd = sd(years_as_adult)) ## # A tibble: 2 x 3 ## gender mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 0.5 0.707 ## 2 Male 1.5 2.12 "],
["local-data-import-and-export.html", "6 Local Data Import and Export 6.1 Basic exportation to a .csv file 6.2 Reading in rectangular data 6.3 Importing data from other programs", " 6 Local Data Import and Export In this chapter we will address the following issues: How to write data to a spreadsheet (.csv file) How to read in a simple spreadsheet as a dataframe How to read in SPSS files 6.1 Basic exportation to a .csv file As we discussed in the previous chapter, rectangular data (where rows are observations and columns are attributes of those observations) have a special status in data analysis. They are simple to construct, manipulate, and analyze. For those reasons, a vast majority of the time they will be all you need. R knows this, so it has several functions designed specifically for data in this format (e.g., all the special things you can do with a dataframe). For example, let’s imagine you wanted to export the data we used in the previous chapter (which we can reload here, for convenience). df &lt;- data.frame( gender = c(&#39;Male&#39;, &#39;Female&#39;, &#39;Female&#39;, &#39;Male&#39;, &#39;Female&#39;), shoe = c(&#39;Nikes&#39;, &#39;Reebok&#39;, &#39;Adididas&#39;, &#39;Manolos&#39;, &#39;Nikes&#39;), age = c(18, NA, 19, 21, 18), stringsAsFactors = FALSE) df ## gender shoe age ## 1 Male Nikes 18 ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 4 Male Manolos 21 ## 5 Female Nikes 18 We can write that dataframe to a comma-seperated values (“.csv”) file, using R’s write.csv() function. This function takes two main arguments: the data you want to export and the filename you want your exported data to have (don’t forget to add “.csv” at the end!). write.csv(df, file = &#39;my_shoe_data.csv&#39;) To see what this did, go find the file that R created and try to open it with a spreadsheet program (e.g., Excel). If you don’t know where to look, remember you can use the getwd() command to ask R where it is saving files. getwd() 6.1.1 Other options If you’ve looked at the exported file, you’ve probably noticed there is a new column on the left that wasn’t in our original dataframe. Those are the rownames for your dataframe. Usually they are just numbers, but you can change them to whatever you want. Almost always, they just get in the way. To make sure they don’t get exported along with your other data, you can set rownames = F in your write.csv() call. write.csv(df, file = &#39;my_shoe_data.csv&#39;, row.names = F) Note that this will over-write the existing file! Of course, that is usually what you want, but just make sure you are not overwriting something you wanted to save. 6.2 Reading in rectangular data Most of the time, rectangular data come in the form of a .csv file. This is the kind of file you just created in the last section. It has a simple structure, where all rows occurr on seperate lines of the file and columns within those rows are seperated by commas. To see what they look like “under the hood”, try opening the .csv file from the last section in a text editor, like Notepad or RStudio’s built in text editor. 6.2.1 Traditional importation To read a .csv file into R, we simply use the read.csv() command, storing the result to a dataframe. df &lt;- read.csv(&#39;my_shoe_data.csv&#39;) df ## gender shoe age ## 1 Male Nikes 18 ## 2 Female Reebok NA ## 3 Female Adididas 19 ## 4 Male Manolos 21 ## 5 Female Nikes 18 6.2.2 Importation options to know about Note there are a few important parameters to this function we can alter here, if we want. You can see them using the ?read.csv help file, but I’ll summarize the big ones here. header - should the first line of the incoming file (the “header”) be treated as the variable names? This defaults to TRUE because it is almost always the case, but you should be aware of it, in case your incoming data looks different. sep - This is the symbol R will use to differentiate columns. It is nearly always a “,” but you can change it if there is something special about your data. na.strings - The symbol R will use to denote NA (missing) data. Often, people do silly things like using “-999” to code missing data. After you are done feeling ashamed of them for that very irresponsible decision, you can import their data by setting na.strings = '-999', which will convert all those values to NA during the import. skip - sometimes your data don’t begin on the first line of the incoming file. You can tell R to “skip” those unimportant lines by incrementing the numeric value of this parameter. stringsAsFactors - we talked about this during dataframe creation in the previous chapter. R defaults to bringing all string variables in as categorical. This used to make sense, but in the modern world is kind of a nuisance. Most often, it is best to set to FALSE. 6.2.3 Consider the readr package As the last section might have implied, there are several options you can change to influence how your .csvs are imported. Some of them default to values that don’t really make sense anymore (e.g., stringsAsFactors = TRUE). The readr package attempts to resolve some of this, by offering the same set of importation functions as base R, but providing more sensible defaults. To use this package, install the tidyverse, which contains readr in it. install.packages(&#39;tidyverse&#39;) Then load readr and use the read_csv() function. library(readr) df &lt;- read_csv(&#39;my_shoe_data.csv&#39;) ## Parsed with column specification: ## cols( ## gender = col_character(), ## shoe = col_character(), ## age = col_double() ## ) Notice that read_csv() did not need to be told your strings were not factors, as it’s default behavior is to assume you wanted character variables. Both readr’s read_csv() function and base R’s read.csv() will generally work for your purposes, but read_csv() is generally more reliable, so it is worth making your default. 6.3 Importing data from other programs Sometimes, we want to import data that are more than just simple numbers, letters, and commas. For example, the data might come in SPSS format. To see how this is done, let’s first download some example data. In this case, we’ll be looking at some exam scores formatted as a .sav (SPSS) file. Note, there are many packages that are designed to import non-R files into R. For example, the most popular one in the past was the foreign package, though it has been replaced by more modern packages. My personal favorite is the memisc package, which offers substantial control over how the importation process occurs. When working with large datasets, this can be very important. However, this package would probably be like using a sledge hammer to pound a nail, when working with a small dataset. For our purposes today, we will use the haven package. If you haven’t installed it already, make sure to do so. install.packages(&#39;haven&#39;) Then load the library and use the read_sav() function. library(haven) df &lt;- read_sav(&#39;exam.sav&#39;) Now we can use the head() function to look at the “head” (first six lines) of our newly imported dataset. head(df) ## # A tibble: 6 x 4 ## ID gender exam scores ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 f 1 45 ## 2 1 f 2 33 ## 3 1 f 3 29.5 ## 4 2 f 1 44 ## 5 2 f 2 33 ## 6 2 f 3 27 "],
["data-joins.html", "7 Data joins 7.1 Left joins 7.2 Inner joins 7.3 Full joins 7.4 Anti joins", " 7 Data joins In this chapter we work to make one of the most tedious parts of data manipulation easier: merging (“joining”) datasets. We will cover four kinds of merges. Left joins Inner joins Full joins Anti joins Over the course of the chapter, we’ll be working with the following data. At various point’s we’ll add and subtract from these basic values to show what happens for each type of join when information is missing from one or the other of the datasets. participants &lt;- c(&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charlie&#39;, &#39;Dayvon&#39;, &#39;Eve&#39;) ages &lt;- c(25, 26, 27, 28, 29) incomes &lt;- c(10000, 20000, 30000, 40000, 50000) 7.1 Left joins These are likely the most common joins. If you have two datasets (call them “a” and “b”), a left join looks for all the records on right (b) that match records on the left (a), then inserts the matches into the left dataset (a). library(tidyverse) df_a &lt;- data.frame(name = participants, age = ages) # Eve is missing from the second dataframe df_b &lt;- data.frame(name = participants[1:4], income = incomes[1:4]) df_left_joined &lt;- left_join(df_a, df_b, by = &#39;name&#39;) ## Warning: Column `name` joining factors with different levels, coercing to ## character vector df_left_joined ## name age income ## 1 Alice 25 10000 ## 2 Bob 26 20000 ## 3 Charlie 27 30000 ## 4 Dayvon 28 40000 ## 5 Eve 29 NA 7.2 Inner joins Inner joins look for records that match in both A and B, then return only the rows that match both. df_a &lt;- data.frame(name = participants, age = ages) # Eve is missing from the second dataframe df_b &lt;- data.frame(name = participants[1:4], income = incomes[1:4]) df_inner_joined &lt;- inner_join(df_a, df_b, by = &#39;name&#39;) ## Warning: Column `name` joining factors with different levels, coercing to ## character vector df_inner_joined ## name age income ## 1 Alice 25 10000 ## 2 Bob 26 20000 ## 3 Charlie 27 30000 ## 4 Dayvon 28 40000 7.3 Full joins Full joins look for records that match in both A and B. They then match everything they can, but return all the rows. # Alice is missing from the first df_a &lt;- data.frame(name = participants[2:5], age = ages[2:5]) # Eve is missing from the second dataframe df_b &lt;- data.frame(name = participants[1:4], income = incomes[1:4]) df_full_joined &lt;- full_join(df_a, df_b, by = &#39;name&#39;) ## Warning: Column `name` joining factors with different levels, coercing to ## character vector df_full_joined ## name age income ## 1 Bob 26 20000 ## 2 Charlie 27 30000 ## 3 Dayvon 28 40000 ## 4 Eve 29 NA ## 5 Alice NA 10000 7.4 Anti joins These are a type of “filtering” join, named such because they are usually used to filter rows you want to find, rather than to create a new dataset. Anti joins look for all of the rows in A that don’t match in B, then return only non-matching rows. This is useful if you want, for example, to find people who hadn’t completed the second part of a data collection study yet. # Alice is missing from the first df_a &lt;- data.frame(name = participants, age = ages) # Eve is missing from the second dataframe df_b &lt;- data.frame(name = participants[1:4], income = incomes[1:4]) df_anti_joined &lt;- anti_join(df_a, df_b, by = &#39;name&#39;) ## Warning: Column `name` joining factors with different levels, coercing to ## character vector df_anti_joined ## name age ## 1 Eve 29 "],
["reshaping-data-with-tidyr.html", "8 Reshaping data with tidyr 8.1 Turning long data into wide 8.2 Turning wide data into long 8.3 Other examples we talked about", " 8 Reshaping data with tidyr One of the most common problems in data management that lies just beyond most people’s grasp is reshaping data. In this chapter, we will cover two basic issues. Turning “long” data into “wide” data Turning “wide” data into “long” data If you are interested in a deeper understanding of how the shape of your data can improve the way you think about your analysis, Hadley Wickam’s paper on the subject is an excellent read. 8.1 Turning long data into wide Imagine you have an undergraduate research assistant who is trying to make life easy in a longitudinal study. In that study, participants return for a total of 3 times to retake the same test. You will eventually need each participant to have four columns - one for their name and three for each of their scores. However, the assistant has simply added a new row each time a participant returns, resulting in a dataset that looks like this: library(tidyverse) df &lt;- expand.grid(id = c(&#39;Alice&#39;, &#39;Bob&#39;, &#39;Chantal&#39;), time = c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)) %&gt;% arrange(id, time) df$score &lt;- sample(1:nrow(df)) df ## id time score ## 1 Alice a 5 ## 2 Alice b 3 ## 3 Alice c 7 ## 4 Bob a 9 ## 5 Bob b 2 ## 6 Bob c 4 ## 7 Chantal a 1 ## 8 Chantal b 8 ## 9 Chantal c 6 To turn them into the format we want, we use the spread() function from the tidyverse sub-package “tidyr”. spread_df &lt;- df %&gt;% spread(time, score) spread_df ## id a b c ## 1 Alice 5 3 7 ## 2 Bob 9 2 4 ## 3 Chantal 1 8 6 If I want to add the names of my key variable to my new column variables, I simply set the sep parameter (check the help files) to add the name of my variable to the new columns. df %&gt;% spread(time, score, sep = &#39;_&#39;) ## id time_a time_b time_c ## 1 Alice 5 3 7 ## 2 Bob 9 2 4 ## 3 Chantal 1 8 6 8.2 Turning wide data into long To turn wide data back into long data, we use the gather() function from the same package. Note that gather() and spread() are exact opposites. long_df &lt;- spread_df %&gt;% gather(time, score) ## Warning: attributes are not identical across measure variables; ## they will be dropped long_df ## time score ## 1 id Alice ## 2 id Bob ## 3 id Chantal ## 4 a 5 ## 5 a 9 ## 6 a 1 ## 7 b 3 ## 8 b 2 ## 9 b 8 ## 10 c 7 ## 11 c 4 ## 12 c 6 Uh oh! We accidentally folded too much information into long format. We lost our id’s into the time and score variables. This is because we forgot to tell gather() whether there were any columns that we didn’t want to be involved in the process. In this case, we want our id’s to stay our of the affair, so we add a - symbol in front of them. long_df &lt;- spread_df %&gt;% gather(time, score, -id) long_df ## id time score ## 1 Alice a 5 ## 2 Bob a 9 ## 3 Chantal a 1 ## 4 Alice b 3 ## 5 Bob b 2 ## 6 Chantal b 8 ## 7 Alice c 7 ## 8 Bob c 4 ## 9 Chantal c 6 Now, our data are returned to their original form. 8.3 Other examples we talked about library(tidyverse) df &lt;- expand.grid( id = c(&#39;Alice&#39;, &#39;Bob&#39;, &#39;Chantal&#39;), time = c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)) %&gt;% arrange(id, time) %&gt;% mutate(score = sample(1:nrow(df))) df ## id time score ## 1 Alice a 7 ## 2 Alice b 1 ## 3 Alice c 9 ## 4 Bob a 5 ## 5 Bob b 2 ## 6 Bob c 8 ## 7 Chantal a 3 ## 8 Chantal b 4 ## 9 Chantal c 6 spread_df &lt;- df %&gt;% spread(time, score, sep = &#39;_&#39;) spread_df ## id time_a time_b time_c ## 1 Alice 7 1 9 ## 2 Bob 5 2 8 ## 3 Chantal 3 4 6 spread_df %&gt;% gather(time, score, -id) %&gt;% group_by(id) %&gt;% mutate(center_score = score - mean(score)) %&gt;% ungroup() %&gt;% arrange(id, time) ## # A tibble: 9 x 4 ## id time score center_score ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Alice time_a 7 1.33 ## 2 Alice time_b 1 -4.67 ## 3 Alice time_c 9 3.33 ## 4 Bob time_a 5 0 ## 5 Bob time_b 2 -3 ## 6 Bob time_c 8 3 ## 7 Chantal time_a 3 -1.33 ## 8 Chantal time_b 4 -0.333 ## 9 Chantal time_c 6 1.67 "],
["regression-models.html", "9 Regression Models 9.1 Simulating fake data 9.2 Fitting regression models 9.3 Other ways of viewing the output 9.4 Fitting multiple regressions simultaneously", " 9 Regression Models Now that we have a grasp on data manipulation, it’s time to turn to data analysis. In this chapter, we focus on some of the most common statistical models in use today: linear regression and logistic regression. To get a sense for how to estimate these models, we will… Simulate fake data to analyze Fit (estimate) both linear and logistic regression models Look at different ways to view the results, using base R and broom Using the tidyverse to fit several models simultaneously 9.1 Simulating fake data Before we fit our first regression model, lets simulate some data to analyze. We’ll do that by first simulating two normally distributed predictors. Then, we’ll choose some slopes for those predictors and make an outcome variable equal to the predictors times their slopes, plus some random error. To give us a binary outcome, we will also make a seperate variable in which our original y is dichotomized at the median with a little bit of error(y_dichot). library(tidyverse) set.seed(314159) n_cases &lt;- 1000 df &lt;- data.frame(id = 1:n_cases) %&gt;% mutate( x1 = rnorm(n_cases, mean = 10, sd = 1), x2 = rnorm(n_cases, mean = 20, sd = 2), y = 5.00 + .30*x1 + .60*x2 + rnorm(n_cases, mean = 0, sd = 1), y_dichot = as.numeric(y &gt; median(y) + rnorm(n_cases))) Here, we have an intercept of \\(5.00\\), a slope of \\(.30\\) for \\(x_1\\), slope of \\(.60\\) for \\(x_2\\). Our outcome \\(y\\) has an error variance of \\(1.0\\). Note that there is nothing special about these numbers, I just made them up because we needed some. If our linear regression model does a good job, it should find an intercept and slopes close (though not exactly equal) to these. When we use our y_dichot outcome in a logistic regression, note that the numbers won’t be exactly the same because the slopes will be on a logrithmic scale. Our resulting data look like this. head(df) ## id x1 x2 y y_dichot ## 1 1 9.200711 23.56806 21.54965 1 ## 2 2 9.269903 21.93050 22.15515 1 ## 3 3 11.436877 21.96769 21.18828 1 ## 4 4 10.305023 21.97102 21.87011 1 ## 5 5 9.602716 21.86214 20.25078 0 ## 6 6 10.088890 17.72826 18.98869 0 9.2 Fitting regression models 9.2.1 Linear regression Now that we’ve got our data, we can fit our first regression model. In R, we fit linear regression models that using the lm() function (“lm” for “linear model”). This function takes two main parameters: a regression formula and the data to which you want that formula applied. The formula is specified using the &lt;outcome&gt; ~ &lt;predictor1&gt; + &lt;predictor2&gt; + ... format. Your outcome goes on the left side of the formula, followed by a squiggle (~), followed by each of your predictors seperated by a +. The data is just your dataframe of interest. lm(formula = y ~ x1 + x2, data = df) ## ## Call: ## lm(formula = y ~ x1 + x2, data = df) ## ## Coefficients: ## (Intercept) x1 x2 ## 4.5117 0.3334 0.6096 Notice that our output is pretty close to what we would have predicted, especially for the two slope coefficients for x1 and x2. But where are our standard errors and p-values? To get them, we need to use the summary() command. To make sure we can call summary again without needing to re-estimate the model, we’ll first save it in a variable called fit. fit &lt;- lm(formula = y ~ x1 + x2, data = df) summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7600 -0.6472 -0.0365 0.6566 2.9302 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.51175 0.42996 10.49 &lt;2e-16 *** ## x1 0.33342 0.03065 10.88 &lt;2e-16 *** ## x2 0.60961 0.01524 40.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9812 on 997 degrees of freedom ## Multiple R-squared: 0.6342, Adjusted R-squared: 0.6335 ## F-statistic: 864.4 on 2 and 997 DF, p-value: &lt; 2.2e-16 There we go! Now I have all the usual information I would need for a regression table. Note that in this case, our p-values (in the column labelled “Pr(&gt;|t|)”) are so small that they need to be given in scientific notation - that’s because our simulated data have very little error in them. Under more normal circumstances, your p-values will likely be much larger. 9.2.2 Logistic regression Fitting a logistic regression model is almost exactly the same, except for a few important differences. First, we will use our dichotomized variable (y_dichot) as the outcome in our regression formula. Additionally, we will use the glm() function (“glm” for “generalized linear model”) and we need to specify what family of generalized linear model we are using in that function. For a logistic regression that family = 'binomial'. glm(y_dichot ~ x1 + x2, data = df, family = &#39;binomial&#39;) ## ## Call: glm(formula = y_dichot ~ x1 + x2, family = &quot;binomial&quot;, data = df) ## ## Coefficients: ## (Intercept) x1 x2 ## -20.4650 0.4857 0.7832 ## ## Degrees of Freedom: 999 Total (i.e. Null); 997 Residual ## Null Deviance: 1386 ## Residual Deviance: 999.3 AIC: 1005 Once again, calling the model function alone gives us some important information, but not very much. So, we’ll follow the same trick above and use the summary function. fit2 &lt;- glm(y_dichot ~ x1 + x2, data = df, family = &#39;binomial&#39;) summary(fit2) ## ## Call: ## glm(formula = y_dichot ~ x1 + x2, family = &quot;binomial&quot;, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2937 -0.7904 0.1182 0.8190 2.4370 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -20.46501 1.43852 -14.226 &lt; 2e-16 *** ## x1 0.48566 0.07961 6.101 1.06e-09 *** ## x2 0.78319 0.05252 14.912 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1386.29 on 999 degrees of freedom ## Residual deviance: 999.32 on 997 degrees of freedom ## AIC: 1005.3 ## ## Number of Fisher Scoring iterations: 5 That’s once again much better! Now we have all of the information we would normally need to report in a regression table. 9.3 Other ways of viewing the output The summary() function from R is nice for looking at our results in real time, but it can be a real pain if I want to modify or export the output. For example, what if I wanted to test whether my results were still significant after Bonferroni correction? For a small number of variables, we could just eyeball it. But for any reasonably large regression, we should be using code to do it instead. To get your output in a more code-friendly format, you can install and use the broom package. install.packages(&#39;broom&#39;) This package has 3 major functions designed to help you out with standard regression models: glance(), tidy(), and augment(). 9.3.1 glance() If I want to see some basic model information (e.g., model sum of squares, \\(R^2\\)), I call glance() on my fit object. This function takes my fit object and gives me model summary information in a one-row dataframe. library(broom) glance(fit) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.634 0.634 0.981 864. 1.77e-218 3 -1398. 2805. ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; If I want to see some more specific information about the coefficients in my model, I call tidy(). tidy(fit) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.51 0.430 10.5 1.66e- 24 ## 2 x1 0.333 0.0306 10.9 3.95e- 26 ## 3 x2 0.610 0.0152 40.0 1.63e-209 Now if I want to adjust my p-values, I can easily do so with the tidyverse library(tidyverse) fit %&gt;% tidy() %&gt;% mutate(adj_p = p.adjust(p.value, method = &#39;bonferroni&#39;)) ## # A tibble: 3 x 6 ## term estimate std.error statistic p.value adj_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.51 0.430 10.5 1.66e- 24 4.98e- 24 ## 2 x1 0.333 0.0306 10.9 3.95e- 26 1.18e- 25 ## 3 x2 0.610 0.0152 40.0 1.63e-209 4.90e-209 Lastly, if I want to see some participant-level information about my model (e.g., predicted values), I can use augment(). This gives me a ton of useful information, including predicted values and standardized residuals. augment(fit) ## # A tibble: 1,000 x 10 ## y x1 x2 .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.5 9.20 23.6 21.9 0.0679 -0.397 0.00478 0.982 2.64e-4 ## 2 22.2 9.27 21.9 21.0 0.0486 1.18 0.00246 0.981 1.20e-3 ## 3 21.2 11.4 22.0 21.7 0.0621 -0.528 0.00401 0.982 3.91e-4 ## 4 21.9 10.3 22.0 21.3 0.0447 0.529 0.00208 0.982 2.02e-4 ## 5 20.3 9.60 21.9 21.0 0.0442 -0.790 0.00203 0.981 4.40e-4 ## 6 19.0 10.1 17.7 18.7 0.0461 0.306 0.00221 0.982 7.18e-5 ## 7 20.1 11.2 20.1 20.5 0.0478 -0.384 0.00237 0.982 1.22e-4 ## 8 20.5 9.22 19.4 19.4 0.0395 1.10 0.00162 0.981 6.77e-4 ## 9 24.3 8.24 23.2 21.4 0.0791 2.93 0.00649 0.977 1.96e-2 ## 10 20.0 7.70 21.0 19.9 0.0784 0.0792 0.00638 0.982 1.40e-5 ## # … with 990 more rows, and 1 more variable: .std.resid &lt;dbl&gt; 9.4 Fitting multiple regressions simultaneously Sometimes, we’ll have a bunch of groups that we want to analyze seperately. For one or two groups, this isn’t so bad to do individually. However, for larger numbers of groups, we’ll need something more advanced. For example, let’s assign people to some groups in our existing dataset. df &lt;- df %&gt;% mutate(grp = sample(letters, size = nrow(.), replace = T)) head(df) ## id x1 x2 y y_dichot grp ## 1 1 9.200711 23.56806 21.54965 1 c ## 2 2 9.269903 21.93050 22.15515 1 v ## 3 3 11.436877 21.96769 21.18828 1 l ## 4 4 10.305023 21.97102 21.87011 1 y ## 5 5 9.602716 21.86214 20.25078 0 i ## 6 6 10.088890 17.72826 18.98869 0 t Here, we’ve assigned people to one of 26 groups (a group for each letter of the alphabet). How can we fit all these regressions separately? With the tidyverse’s group_by() and do() functions. model_df &lt;- df %&gt;% group_by(grp) %&gt;% do(model = lm(y ~ x1 + x2, data = .)) head(model_df) ## Source: local data frame [6 x 2] ## Groups: &lt;by row&gt; ## ## # A tibble: 6 x 2 ## grp model ## &lt;chr&gt; &lt;list&gt; ## 1 a &lt;S3: lm&gt; ## 2 b &lt;S3: lm&gt; ## 3 c &lt;S3: lm&gt; ## 4 d &lt;S3: lm&gt; ## 5 e &lt;S3: lm&gt; ## 6 f &lt;S3: lm&gt; If we want to unpack those models, we can once again use broom’s functions. We just need to make sure to specify the column where our models are, which in this case is “model”. coef_table &lt;- model_df %&gt;% tidy(model) head(coef_table, n = 10) ## # A tibble: 10 x 6 ## # Groups: grp [4] ## grp term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a (Intercept) 8.16 2.52 3.24 2.94e- 3 ## 2 a x1 0.231 0.164 1.41 1.69e- 1 ## 3 a x2 0.486 0.0793 6.13 9.80e- 7 ## 4 b (Intercept) 4.72 2.35 2.00 5.17e- 2 ## 5 b x1 0.209 0.141 1.48 1.48e- 1 ## 6 b x2 0.682 0.0789 8.64 9.05e-11 ## 7 c (Intercept) 5.05 2.43 2.08 4.31e- 2 ## 8 c x1 0.425 0.149 2.86 6.53e- 3 ## 9 c x2 0.539 0.0978 5.52 1.72e- 6 ## 10 d (Intercept) 6.56 2.10 3.12 3.16e- 3 We can do the same thing for summary model information with the glance() function. model_table &lt;- model_df %&gt;% glance(model) head(model_table) ## # A tibble: 6 x 12 ## # Groups: grp [6] ## grp r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 0.556 0.526 0.883 18.8 5.18e- 6 3 -41.1 90.3 ## 2 b 0.646 0.629 0.983 37.4 5.72e-10 3 -60.1 128. ## 3 c 0.464 0.440 1.03 19.1 1.09e- 6 3 -66.4 141. ## 4 d 0.551 0.531 1.01 27.6 1.48e- 8 3 -67.1 142. ## 5 e 0.814 0.801 0.763 63.3 2.63e-11 3 -35.2 78.4 ## 6 f 0.693 0.677 0.900 41.8 3.20e-10 3 -51.0 110. ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; If we want, we can even see the distribution of things we care about, like \\(R^2\\). ggplot(model_table, aes(r.squared)) + geom_density(fill = &#39;grey&#39;, alpha = .50) "],
["regression-models-1.html", "10 Regression Models 10.1 Simulating fake data 10.2 Fitting regression models 10.3 Other ways of viewing the output 10.4 Fitting multiple regressions simultaneously", " 10 Regression Models Now that we have a grasp on data manipulation, it’s time to turn to data analysis. In this chapter, we focus on some of the most common statistical models in use today: linear regression and logistic regression. To get a sense for how to estimate these models, we will… Simulate fake data to analyze Fit (estimate) both linear and logistic regression models Look at different ways to view the results, using base R and broom Using the tidyverse to fit several models simultaneously 10.1 Simulating fake data Before we fit our first regression model, lets simulate some data to analyze. We’ll do that by first simulating two normally distributed predictors. Then, we’ll choose some slopes for those predictors and make an outcome variable equal to the predictors times their slopes, plus some random error. To give us a binary outcome, we will also make a seperate variable in which our original y is dichotomized at the median with a little bit of error(y_dichot). library(tidyverse) set.seed(314159) n_cases &lt;- 1000 df &lt;- data.frame(id = 1:n_cases) %&gt;% mutate( x1 = rnorm(n_cases, mean = 10, sd = 1), x2 = rnorm(n_cases, mean = 20, sd = 2), y = 5.00 + .30*x1 + .60*x2 + rnorm(n_cases, mean = 0, sd = 1), y_dichot = as.numeric(y &gt; median(y) + rnorm(n_cases))) Here, we have an intercept of \\(5.00\\), a slope of \\(.30\\) for \\(x_1\\), slope of \\(.60\\) for \\(x_2\\). Our outcome \\(y\\) has an error variance of \\(1.0\\). Note that there is nothing special about these numbers, I just made them up because we needed some. If our linear regression model does a good job, it should find an intercept and slopes close (though not exactly equal) to these. When we use our y_dichot outcome in a logistic regression, note that the numbers won’t be exactly the same because the slopes will be on a logrithmic scale. Our resulting data look like this. head(df) ## id x1 x2 y y_dichot ## 1 1 9.200711 23.56806 21.54965 1 ## 2 2 9.269903 21.93050 22.15515 1 ## 3 3 11.436877 21.96769 21.18828 1 ## 4 4 10.305023 21.97102 21.87011 1 ## 5 5 9.602716 21.86214 20.25078 0 ## 6 6 10.088890 17.72826 18.98869 0 10.2 Fitting regression models 10.2.1 Linear regression Now that we’ve got our data, we can fit our first regression model. In R, we fit linear regression models that using the lm() function (“lm” for “linear model”). This function takes two main parameters: a regression formula and the data to which you want that formula applied. The formula is specified using the &lt;outcome&gt; ~ &lt;predictor1&gt; + &lt;predictor2&gt; + ... format. Your outcome goes on the left side of the formula, followed by a squiggle (~), followed by each of your predictors seperated by a +. The data is just your dataframe of interest. lm(formula = y ~ x1 + x2, data = df) ## ## Call: ## lm(formula = y ~ x1 + x2, data = df) ## ## Coefficients: ## (Intercept) x1 x2 ## 4.5117 0.3334 0.6096 Notice that our output is pretty close to what we would have predicted, especially for the two slope coefficients for x1 and x2. But where are our standard errors and p-values? To get them, we need to use the summary() command. To make sure we can call summary again without needing to re-estimate the model, we’ll first save it in a variable called fit. fit &lt;- lm(formula = y ~ x1 + x2, data = df) summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7600 -0.6472 -0.0365 0.6566 2.9302 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.51175 0.42996 10.49 &lt;2e-16 *** ## x1 0.33342 0.03065 10.88 &lt;2e-16 *** ## x2 0.60961 0.01524 40.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9812 on 997 degrees of freedom ## Multiple R-squared: 0.6342, Adjusted R-squared: 0.6335 ## F-statistic: 864.4 on 2 and 997 DF, p-value: &lt; 2.2e-16 There we go! Now I have all the usual information I would need for a regression table. Note that in this case, our p-values (in the column labelled “Pr(&gt;|t|)”) are so small that they need to be given in scientific notation - that’s because our simulated data have very little error in them. Under more normal circumstances, your p-values will likely be much larger. 10.2.2 Logistic regression Fitting a logistic regression model is almost exactly the same, except for a few important differences. First, we will use our dichotomized variable (y_dichot) as the outcome in our regression formula. Additionally, we will use the glm() function (“glm” for “generalized linear model”) and we need to specify what family of generalized linear model we are using in that function. For a logistic regression that family = 'binomial'. glm(y_dichot ~ x1 + x2, data = df, family = &#39;binomial&#39;) ## ## Call: glm(formula = y_dichot ~ x1 + x2, family = &quot;binomial&quot;, data = df) ## ## Coefficients: ## (Intercept) x1 x2 ## -20.4650 0.4857 0.7832 ## ## Degrees of Freedom: 999 Total (i.e. Null); 997 Residual ## Null Deviance: 1386 ## Residual Deviance: 999.3 AIC: 1005 Once again, calling the model function alone gives us some important information, but not very much. So, we’ll follow the same trick above and use the summary function. fit2 &lt;- glm(y_dichot ~ x1 + x2, data = df, family = &#39;binomial&#39;) summary(fit2) ## ## Call: ## glm(formula = y_dichot ~ x1 + x2, family = &quot;binomial&quot;, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2937 -0.7904 0.1182 0.8190 2.4370 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -20.46501 1.43852 -14.226 &lt; 2e-16 *** ## x1 0.48566 0.07961 6.101 1.06e-09 *** ## x2 0.78319 0.05252 14.912 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1386.29 on 999 degrees of freedom ## Residual deviance: 999.32 on 997 degrees of freedom ## AIC: 1005.3 ## ## Number of Fisher Scoring iterations: 5 That’s once again much better! Now we have all of the information we would normally need to report in a regression table. 10.3 Other ways of viewing the output The summary() function from R is nice for looking at our results in real time, but it can be a real pain if I want to modify or export the output. For example, what if I wanted to test whether my results were still significant after Bonferroni correction? For a small number of variables, we could just eyeball it. But for any reasonably large regression, we should be using code to do it instead. To get your output in a more code-friendly format, you can install and use the broom package. install.packages(&#39;broom&#39;) This package has 3 major functions designed to help you out with standard regression models: glance(), tidy(), and augment(). 10.3.1 glance() If I want to see some basic model information (e.g., model sum of squares, \\(R^2\\)), I call glance() on my fit object. This function takes my fit object and gives me model summary information in a one-row dataframe. library(broom) glance(fit) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.634 0.634 0.981 864. 1.77e-218 3 -1398. 2805. ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; If I want to see some more specific information about the coefficients in my model, I call tidy(). tidy(fit) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.51 0.430 10.5 1.66e- 24 ## 2 x1 0.333 0.0306 10.9 3.95e- 26 ## 3 x2 0.610 0.0152 40.0 1.63e-209 Now if I want to adjust my p-values, I can easily do so with the tidyverse library(tidyverse) fit %&gt;% tidy() %&gt;% mutate(adj_p = p.adjust(p.value, method = &#39;bonferroni&#39;)) ## # A tibble: 3 x 6 ## term estimate std.error statistic p.value adj_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.51 0.430 10.5 1.66e- 24 4.98e- 24 ## 2 x1 0.333 0.0306 10.9 3.95e- 26 1.18e- 25 ## 3 x2 0.610 0.0152 40.0 1.63e-209 4.90e-209 Lastly, if I want to see some participant-level information about my model (e.g., predicted values), I can use augment(). This gives me a ton of useful information, including predicted values and standardized residuals. augment(fit) ## # A tibble: 1,000 x 10 ## y x1 x2 .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.5 9.20 23.6 21.9 0.0679 -0.397 0.00478 0.982 2.64e-4 ## 2 22.2 9.27 21.9 21.0 0.0486 1.18 0.00246 0.981 1.20e-3 ## 3 21.2 11.4 22.0 21.7 0.0621 -0.528 0.00401 0.982 3.91e-4 ## 4 21.9 10.3 22.0 21.3 0.0447 0.529 0.00208 0.982 2.02e-4 ## 5 20.3 9.60 21.9 21.0 0.0442 -0.790 0.00203 0.981 4.40e-4 ## 6 19.0 10.1 17.7 18.7 0.0461 0.306 0.00221 0.982 7.18e-5 ## 7 20.1 11.2 20.1 20.5 0.0478 -0.384 0.00237 0.982 1.22e-4 ## 8 20.5 9.22 19.4 19.4 0.0395 1.10 0.00162 0.981 6.77e-4 ## 9 24.3 8.24 23.2 21.4 0.0791 2.93 0.00649 0.977 1.96e-2 ## 10 20.0 7.70 21.0 19.9 0.0784 0.0792 0.00638 0.982 1.40e-5 ## # … with 990 more rows, and 1 more variable: .std.resid &lt;dbl&gt; 10.4 Fitting multiple regressions simultaneously Sometimes, we’ll have a bunch of groups that we want to analyze seperately. For one or two groups, this isn’t so bad to do individually. However, for larger numbers of groups, we’ll need something more advanced. For example, let’s assign people to some groups in our existing dataset. df &lt;- df %&gt;% mutate(grp = sample(letters, size = nrow(.), replace = T)) head(df) ## id x1 x2 y y_dichot grp ## 1 1 9.200711 23.56806 21.54965 1 c ## 2 2 9.269903 21.93050 22.15515 1 v ## 3 3 11.436877 21.96769 21.18828 1 l ## 4 4 10.305023 21.97102 21.87011 1 y ## 5 5 9.602716 21.86214 20.25078 0 i ## 6 6 10.088890 17.72826 18.98869 0 t Here, we’ve assigned people to one of 26 groups (a group for each letter of the alphabet). How can we fit all these regressions separately? With the tidyverse’s group_by() and do() functions. model_df &lt;- df %&gt;% group_by(grp) %&gt;% do(model = lm(y ~ x1 + x2, data = .)) head(model_df) ## Source: local data frame [6 x 2] ## Groups: &lt;by row&gt; ## ## # A tibble: 6 x 2 ## grp model ## &lt;chr&gt; &lt;list&gt; ## 1 a &lt;S3: lm&gt; ## 2 b &lt;S3: lm&gt; ## 3 c &lt;S3: lm&gt; ## 4 d &lt;S3: lm&gt; ## 5 e &lt;S3: lm&gt; ## 6 f &lt;S3: lm&gt; If we want to unpack those models, we can once again use broom’s functions. We just need to make sure to specify the column where our models are, which in this case is “model”. coef_table &lt;- model_df %&gt;% tidy(model) head(coef_table, n = 10) ## # A tibble: 10 x 6 ## # Groups: grp [4] ## grp term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a (Intercept) 8.16 2.52 3.24 2.94e- 3 ## 2 a x1 0.231 0.164 1.41 1.69e- 1 ## 3 a x2 0.486 0.0793 6.13 9.80e- 7 ## 4 b (Intercept) 4.72 2.35 2.00 5.17e- 2 ## 5 b x1 0.209 0.141 1.48 1.48e- 1 ## 6 b x2 0.682 0.0789 8.64 9.05e-11 ## 7 c (Intercept) 5.05 2.43 2.08 4.31e- 2 ## 8 c x1 0.425 0.149 2.86 6.53e- 3 ## 9 c x2 0.539 0.0978 5.52 1.72e- 6 ## 10 d (Intercept) 6.56 2.10 3.12 3.16e- 3 We can do the same thing for summary model information with the glance() function. model_table &lt;- model_df %&gt;% glance(model) head(model_table) ## # A tibble: 6 x 12 ## # Groups: grp [6] ## grp r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 0.556 0.526 0.883 18.8 5.18e- 6 3 -41.1 90.3 ## 2 b 0.646 0.629 0.983 37.4 5.72e-10 3 -60.1 128. ## 3 c 0.464 0.440 1.03 19.1 1.09e- 6 3 -66.4 141. ## 4 d 0.551 0.531 1.01 27.6 1.48e- 8 3 -67.1 142. ## 5 e 0.814 0.801 0.763 63.3 2.63e-11 3 -35.2 78.4 ## 6 f 0.693 0.677 0.900 41.8 3.20e-10 3 -51.0 110. ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; If we want, we can even see the distribution of things we care about, like \\(R^2\\). ggplot(model_table, aes(r.squared)) + geom_density(fill = &#39;grey&#39;, alpha = .50) "],
["anova-and-other-tests.html", "11 ANOVA and other tests 11.1 t-Tests 11.2 Correlation 11.3 ANOVA", " 11 ANOVA and other tests In this chapter, we cover other common tests you will have run in other programs. In the later part of the chapter, we will focus specifically on Analysis of Variance (ANOVA). At the outset, let me begin with an apology: R’s ANOVA capabilities are much less intuitive and user friendly than in other programs. What I provide below is my best attempt to make the system understandable. Our agenda includes… t-Tests Correlation ANOVA - one-way and two-way We’ll start by simulating a dataset to use throughout our analyses. Because we have so many tests to run, this dataset will be a little more complex than in past chapters. library(tidyverse) set.seed(314159) n_obs &lt;- 350 df &lt;- data.frame(id = 1:n_obs) %&gt;% mutate( gender = as.factor(sample(c(&#39;male&#39;, &#39;female&#39;), size = n_obs, replace = T)), race = as.factor(sample(c(&#39;Black&#39;, &#39;White&#39;, &#39;Asian&#39;), size = n_obs, replace = T))) %&gt;% group_by(gender, race) %&gt;% mutate( group_age_mean = rnorm(1, sd = 3), age = round(group_age_mean + rnorm(n(), mean = 22, sd = 2), 4), gpa1 = round(runif(n(), 2, 4), 1), gpa2 = round(gpa1 + (4 - gpa1)*runif(n(), min = -.50, max = .75), 1), gpa3 = round(gpa2 + (4 - gpa2)*runif(n(), min = -.50, max = .75), 1)) %&gt;% ungroup() head(df) ## # A tibble: 6 x 8 ## id gender race group_age_mean age gpa1 gpa2 gpa3 ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 male Asian -3.48 22.9 3.7 3.9 4 ## 2 2 female Asian 0.728 21.8 3.1 2.7 2.4 ## 3 3 male White 4.18 26.5 3.2 3.1 3.7 ## 4 4 male White 4.18 26.1 2.5 2.2 3.4 ## 5 5 female White -1.27 18.7 3.2 3 3.1 ## 6 6 female Black -1.10 21.8 2 2.1 2.4 Here, we’ve created a few categorical variables (this time leaving them as factors!) and two continuous variables we might treat as outcomes (i.e., age and GPA at 3 time points.) In this case, a person’s age in this sample is related to both their gender and race, so we should see a two-way interaction (or close to it) with these variables. 11.1 t-Tests To run an independant-samples t-test, we can use the t.test() function. This function expects us to have two columns of data, one for each sample. If we want to compare men and women’s ages in our sample, we’ll simply need to extract those columns of data. t.test(df$age[df$gender == &#39;female&#39;], df$age[df$gender == &#39;male&#39;]) ## ## Welch Two Sample t-test ## ## data: df$age[df$gender == &quot;female&quot;] and df$age[df$gender == &quot;male&quot;] ## t = -5.8366, df = 287.72, p-value = 1.435e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.682701 -1.329635 ## sample estimates: ## mean of x mean of y ## 21.23809 23.24426 To run a paired t-test, we just set paired = TRUE. Let’s try this on the first two columns of GPA. t.test(df$gpa1, df$gpa2, paired = T) ## ## Paired t-test ## ## data: df$gpa1 and df$gpa2 ## t = -5.7344, df = 349, p-value = 2.121e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.17765731 -0.08691412 ## sample estimates: ## mean of the differences ## -0.1322857 Note that t.tes() in R defaults to using the unequal variances formula, as you can see in the help file, where var.equal = FALSE is diplayed in the USAGE section. 11.2 Correlation To view a set of correlations, all we need is to feed our data to the cor() function. cor(df[, c(&#39;age&#39;, &#39;gpa1&#39;, &#39;gpa2&#39;, &#39;gpa3&#39;)]) ## age gpa1 gpa2 gpa3 ## age 1.00000000 -0.02528349 0.03152321 0.07817196 ## gpa1 -0.02528349 1.00000000 0.75606217 0.63319613 ## gpa2 0.03152321 0.75606217 1.00000000 0.83085250 ## gpa3 0.07817196 0.63319613 0.83085250 1.00000000 But what if I have missing data? In that case, consult the help fil for which of the following otions you want to use for computing your correlations. Set use equal to one of: “everything”, “all.obs”, “complete.obs”, “na.or.complete”, or “pairwise.complete.obs”. missing_df &lt;- df[, c(&#39;age&#39;, &#39;gpa1&#39;, &#39;gpa2&#39;, &#39;gpa3&#39;)] %&gt;% mutate(age = ifelse(runif(nrow(.)) &lt; .50, NA, age)) # 50% of scores randomly missing cor(missing_df) ## age gpa1 gpa2 gpa3 ## age 1 NA NA NA ## gpa1 NA 1.0000000 0.7560622 0.6331961 ## gpa2 NA 0.7560622 1.0000000 0.8308525 ## gpa3 NA 0.6331961 0.8308525 1.0000000 cor(missing_df, use = &#39;complete.obs&#39;) ## age gpa1 gpa2 gpa3 ## age 1.00000000 -0.02571405 -0.03134333 0.01158888 ## gpa1 -0.02571405 1.00000000 0.74597217 0.63061236 ## gpa2 -0.03134333 0.74597217 1.00000000 0.83205760 ## gpa3 0.01158888 0.63061236 0.83205760 1.00000000 11.2.1 Rank correlations You can also change the method argument of cor() to one of “pearson”, “kendall”, or “spearman”. Pearson is the default, but both Kendall and Spearman correlations are easy to compute. cor(missing_df, use = &#39;complete.obs&#39;, method = &#39;spearman&#39;) ## age gpa1 gpa2 gpa3 ## age 1.000000000 0.001222439 -0.05115086 -0.01818023 ## gpa1 0.001222439 1.000000000 0.76137738 0.68085668 ## gpa2 -0.051150856 0.761377383 1.00000000 0.84375110 ## gpa3 -0.018180229 0.680856678 0.84375110 1.00000000 11.2.2 Significance testing There is no quick base R solution for computing significance values for your correlations. However, the rcorr() function of the Hmisc package will compute them if you want. Note that you need to use as.matrix() to convert your dataframe to a matrix before rcorr() will understand it. # If you need to, install the package # install.packages(&#39;Hmisc&#39;) df %&gt;% select(gpa1, gpa2, gpa3) %&gt;% as.matrix() %&gt;% # convert to matrix Hmisc::rcorr() ## gpa1 gpa2 gpa3 ## gpa1 1.00 0.76 0.63 ## gpa2 0.76 1.00 0.83 ## gpa3 0.63 0.83 1.00 ## ## n= 350 ## ## ## P ## gpa1 gpa2 gpa3 ## gpa1 0 0 ## gpa2 0 0 ## gpa3 0 0 11.3 ANOVA For analysis of variance, we use the same “formula” model specification that we did for regression: &lt;outcome&gt; ~ &lt;predictor1&gt; + &lt;predictor2&gt; .... aov(age ~ race, data = df) ## Call: ## aov(formula = age ~ race, data = df) ## ## Terms: ## race Residuals ## Sum of Squares 471.767 3497.170 ## Deg. of Freedom 2 347 ## ## Residual standard error: 3.174634 ## Estimated effects may be unbalanced Like regression, we get some useful information from the aov() function alone, but it is usually more helpful to save it to a variable (often named fit, but you can call it whatever you want). Then, we use the summary() function to get a little more detail. fit &lt;- aov(age ~ race, data = df) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## race 2 472 235.88 23.41 2.92e-10 *** ## Residuals 347 3497 10.08 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 WARNING: The default Sum of Squares that R uses is Type I, which is different from SAS and SPSS (which default to Type III). If you want your results to be consistent with SPSS, then try using the drop1() command. drop1(fit) ## Single term deletions ## ## Model: ## age ~ race ## Df Sum of Sq RSS AIC ## &lt;none&gt; 3497.2 811.62 ## race 2 471.77 3968.9 851.91 11.3.1 Post hoc testing To perform post hoc testing of group differences, the most common choice is the TukeyHSD() function, which takes your fit object and performs the HSD post hoc test. TukeyHSD(fit) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = age ~ race, data = df) ## ## $race ## diff lwr upr p adj ## Black-Asian 1.8986587 0.8932565 2.904061 0.0000352 ## White-Asian 2.7659408 1.8008446 3.731037 0.0000000 ## White-Black 0.8672821 -0.1048972 1.839461 0.0913371 Here, we see that there are significant differences in two of three possible cases. 11.3.2 Two-factors If we want to test two factors at once (e.g., race and gender), we simply add them both to the formula. fit &lt;- aov(age ~ race + gender, data = df) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## race 2 471.8 235.9 26.05 2.89e-11 *** ## gender 1 364.1 364.1 40.21 7.12e-10 *** ## Residuals 346 3133.1 9.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 WARNING: Notice again that because the default here is Type I Sum of Squares, variable order matters! To get your SPSS-consistent values, use drop1(). fit &lt;- aov(age ~ gender + race, data = df) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gender 1 352.2 352.2 38.89 1.31e-09 *** ## race 2 483.7 241.9 26.71 1.63e-11 *** ## Residuals 346 3133.1 9.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fit &lt;- aov(age ~ gender + race, data = df) drop1(fit) ## Single term deletions ## ## Model: ## age ~ gender + race ## Df Sum of Sq RSS AIC ## &lt;none&gt; 3133.1 775.14 ## gender 1 364.11 3497.2 811.62 ## race 2 483.73 3616.8 821.39 11.3.3 P-values Unfortunately, to get p-values for the drop1() command, you need to do the extra leg work yourself. Here is a demo of how you would do it. ss &lt;- fit %&gt;% drop1() %&gt;% as.data.frame() residual_df &lt;- fit$df.residual residual_ms &lt;- ss[&#39;&lt;none&gt;&#39;, &#39;RSS&#39;]/residual_df ss[&#39;&lt;none&gt;&#39;, &#39;Df&#39;] &lt;- fit$df.residual ss[&#39;&lt;none&gt;&#39;, &#39;Sum of Sq&#39;] &lt;- sum(fit$residuals^2) ss %&gt;% mutate( mean_squares = `Sum of Sq`/Df, f_stat = mean_squares/residual_ms, p = pf(f_stat, Df, residual_df, lower.tail = F)) ## Df Sum of Sq RSS AIC mean_squares f_stat p ## 1 346 3133.0564 3133.056 775.1409 9.055076 1.00000 5.000000e-01 ## 2 1 364.1139 3497.170 811.6217 364.113945 40.21103 7.123237e-10 ## 3 2 483.7302 3616.787 821.3928 241.865110 26.71044 1.631703e-11 11.3.4 Interactions Interactions can be specified by the &lt;va1&gt;*&lt;var2&gt; command in the formula. fit &lt;- aov(age ~ gender + race + gender:race, data = df) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gender 1 352.2 352.2 76.28 &lt;2e-16 *** ## race 2 483.7 241.9 52.39 &lt;2e-16 *** ## gender:race 2 1545.0 772.5 167.33 &lt;2e-16 *** ## Residuals 344 1588.1 4.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here, we can see both a significant interaction and significant main effects for gender and race, predicting age in our sample. 11.3.5 Plots Using ggplot is likely your most useful way of comparing groups. ggplot(df, aes(x = race, y = age, fill = gender)) + geom_boxplot() "],
["structural-equation-modeling.html", "12 Structural Equation Modeling 12.1 Confirmatory Factor Analysis 12.2 Full Structural Equation Models 12.3 Additional tricks", " 12 Structural Equation Modeling Here we discuss one of the most versatile methods commonly used by social scientists: Structural Equation Modeling (SEM). If you’re new to SEM, you can think of it as a series of regressions that are all estimated at the same time. Because the variables in each of your regressions are usually correlated, this allows the estimation procedure to “borrow” information from one part of your model and use it to inform the results for another. Our agenda in this chapter focuses on two major cases of SEM: Confirmatory Factor Analysis (CFA) Full Structural Equation Models (regression on latent factors) 12.1 Confirmatory Factor Analysis The first step is to install the most popular (and easy to use!) package for SEM in R: lavaan. Use the usual process for installation. install.packages(&#39;lavaan&#39;) This package provides many convenient datasets for practice analysis. The on we will use today is the HolzingerSwineford1939 dataset, which includes scores on a test of scholastic aptitude. Here there are three supposed “factors” that should be influencing those scores: x1, x2, x3 = visual x4, x5, x6 = textual x7, x8, x9 = speed To load the dataset, we execute the code below library(lavaan) data(HolzingerSwineford1939) cfa_df &lt;- HolzingerSwineford1939 head(cfa_df) ## id sex ageyr agemo school grade x1 x2 x3 x4 x5 ## 1 1 1 13 1 Pasteur 7 3.333333 7.75 0.375 2.333333 5.75 ## 2 2 2 13 7 Pasteur 7 5.333333 5.25 2.125 1.666667 3.00 ## 3 3 2 13 1 Pasteur 7 4.500000 5.25 1.875 1.000000 1.75 ## 4 4 1 13 2 Pasteur 7 5.333333 7.75 3.000 2.666667 4.50 ## 5 5 2 12 2 Pasteur 7 4.833333 4.75 0.875 2.666667 4.00 ## 6 6 2 14 1 Pasteur 7 5.333333 5.00 2.250 1.000000 3.00 ## x6 x7 x8 x9 ## 1 1.2857143 3.391304 5.75 6.361111 ## 2 1.2857143 3.782609 6.25 7.916667 ## 3 0.4285714 3.260870 3.90 4.416667 ## 4 2.4285714 3.000000 5.30 4.861111 ## 5 2.5714286 3.695652 6.30 5.916667 ## 6 0.8571429 4.347826 6.65 7.500000 12.1.1 CFA syntax To build a model, we will need a way to communicate to R what we think the relationship between our observed variables are. In this case, we can specify that a latent factor is driving the correlation between our observed variables with a string in the following format (note the =~ is different than just the ~ that we are used to). # Example command for a single factor fac_syntax &lt;- &#39;latent variable =~ indicator1 + indicator2 + indicator3&#39; To translate this structure to our situation, we simply make a multi-line string. For those of you who have used MPlus before, this should feel relatively familiar cfa_model &lt;- &#39;visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; We now fit and summarize the model in much the same way that we would if we were running a regression. fit &lt;- cfa(cfa_model, data = cfa_df) summary(fit) ## lavaan 0.6-3 ended normally after 35 iterations ## ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 301 ## ## Estimator ML ## Model Fit Test Statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## visual =~ ## x1 1.000 ## x2 0.554 0.100 5.554 0.000 ## x3 0.729 0.109 6.685 0.000 ## textual =~ ## x4 1.000 ## x5 1.113 0.065 17.014 0.000 ## x6 0.926 0.055 16.703 0.000 ## speed =~ ## x7 1.000 ## x8 1.180 0.165 7.152 0.000 ## x9 1.082 0.151 7.155 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## visual ~~ ## textual 0.408 0.074 5.552 0.000 ## speed 0.262 0.056 4.660 0.000 ## textual ~~ ## speed 0.173 0.049 3.518 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.549 0.114 4.833 0.000 ## .x2 1.134 0.102 11.146 0.000 ## .x3 0.844 0.091 9.317 0.000 ## .x4 0.371 0.048 7.779 0.000 ## .x5 0.446 0.058 7.642 0.000 ## .x6 0.356 0.043 8.277 0.000 ## .x7 0.799 0.081 9.823 0.000 ## .x8 0.488 0.074 6.573 0.000 ## .x9 0.566 0.071 8.003 0.000 ## visual 0.809 0.145 5.564 0.000 ## textual 0.979 0.112 8.737 0.000 ## speed 0.384 0.086 4.451 0.000 If we wanted additional information on the fit statistics for the model. We can set fit.measures=T in our summary() call. summary(fit, fit.measures=T) ## lavaan 0.6-3 ended normally after 35 iterations ## ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 301 ## ## Estimator ML ## Model Fit Test Statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Model test baseline model: ## ## Minimum Function Test Statistic 918.852 ## Degrees of freedom 36 ## P-value 0.000 ## ## User model versus baseline model: ## ## Comparative Fit Index (CFI) 0.931 ## Tucker-Lewis Index (TLI) 0.896 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3737.745 ## Loglikelihood unrestricted model (H1) -3695.092 ## ## Number of free parameters 21 ## Akaike (AIC) 7517.490 ## Bayesian (BIC) 7595.339 ## Sample-size adjusted Bayesian (BIC) 7528.739 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.092 ## 90 Percent Confidence Interval 0.071 0.114 ## P-value RMSEA &lt;= 0.05 0.001 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.065 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## visual =~ ## x1 1.000 ## x2 0.554 0.100 5.554 0.000 ## x3 0.729 0.109 6.685 0.000 ## textual =~ ## x4 1.000 ## x5 1.113 0.065 17.014 0.000 ## x6 0.926 0.055 16.703 0.000 ## speed =~ ## x7 1.000 ## x8 1.180 0.165 7.152 0.000 ## x9 1.082 0.151 7.155 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## visual ~~ ## textual 0.408 0.074 5.552 0.000 ## speed 0.262 0.056 4.660 0.000 ## textual ~~ ## speed 0.173 0.049 3.518 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.549 0.114 4.833 0.000 ## .x2 1.134 0.102 11.146 0.000 ## .x3 0.844 0.091 9.317 0.000 ## .x4 0.371 0.048 7.779 0.000 ## .x5 0.446 0.058 7.642 0.000 ## .x6 0.356 0.043 8.277 0.000 ## .x7 0.799 0.081 9.823 0.000 ## .x8 0.488 0.074 6.573 0.000 ## .x9 0.566 0.071 8.003 0.000 ## visual 0.809 0.145 5.564 0.000 ## textual 0.979 0.112 8.737 0.000 ## speed 0.384 0.086 4.451 0.000 12.2 Full Structural Equation Models For a more complex example, let’s consider the case where we have several latent variables, but we expect there to be causal (regression) relationships between them. For that, we will use the PoliticalDemocracy dataset from the lavaan package. This dataset (which you can see more of by running ?PoliticalDemocracy), includes variables related to industrialization in 1960, democratic freedom in 1960, and democratic freedom in 1965. Let’s load the dataset and test a model of these relationships. data(PoliticalDemocracy) sem_df &lt;- PoliticalDemocracy To specify the factors, we will again use the =~ operator. cfa_model &lt;- &#39; # measurement model ind60 =~ x1 + x2 + x3 dem60 =~ y1 + y2 + y3 + y4 dem65 =~ y5 + y6 + y7 + y8&#39; However, when we have regression relationships (prediction, causation), we use the ~ operator instead. path_model &lt;- &#39; # regressions dem60 ~ ind60 dem65 ~ ind60 + dem60&#39; Sometimes, we also want error terms to be correlated. Whenever specifying a variance, covariance, or correlation, we use the ~~ operator. residual_correlations &lt;- &#39; # residual correlations y1 ~~ y5 y2 ~~ y4 + y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8&#39; To put the whole model together, we use paste(). We will set sep = '\\n' inside paste(), which means we want a new line between each of our input strings. full_model &lt;- paste(cfa_model, path_model, residual_correlations, sep = &#39;\\n&#39;) Now let’s run and summarize that model, in the same way we did with our CFA. The only difference is we use the sem() function instead. fit &lt;- sem(full_model, data = sem_df) summary(fit, fit.measures = T) ## lavaan 0.6-3 ended normally after 68 iterations ## ## Optimization method NLMINB ## Number of free parameters 31 ## ## Number of observations 75 ## ## Estimator ML ## Model Fit Test Statistic 38.125 ## Degrees of freedom 35 ## P-value (Chi-square) 0.329 ## ## Model test baseline model: ## ## Minimum Function Test Statistic 730.654 ## Degrees of freedom 55 ## P-value 0.000 ## ## User model versus baseline model: ## ## Comparative Fit Index (CFI) 0.995 ## Tucker-Lewis Index (TLI) 0.993 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1547.791 ## Loglikelihood unrestricted model (H1) -1528.728 ## ## Number of free parameters 31 ## Akaike (AIC) 3157.582 ## Bayesian (BIC) 3229.424 ## Sample-size adjusted Bayesian (BIC) 3131.720 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.035 ## 90 Percent Confidence Interval 0.000 0.092 ## P-value RMSEA &lt;= 0.05 0.611 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.044 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## ind60 =~ ## x1 1.000 ## x2 2.180 0.139 15.742 0.000 ## x3 1.819 0.152 11.967 0.000 ## dem60 =~ ## y1 1.000 ## y2 1.257 0.182 6.889 0.000 ## y3 1.058 0.151 6.987 0.000 ## y4 1.265 0.145 8.722 0.000 ## dem65 =~ ## y5 1.000 ## y6 1.186 0.169 7.024 0.000 ## y7 1.280 0.160 8.002 0.000 ## y8 1.266 0.158 8.007 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dem60 ~ ## ind60 1.483 0.399 3.715 0.000 ## dem65 ~ ## ind60 0.572 0.221 2.586 0.010 ## dem60 0.837 0.098 8.514 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1 ~~ ## .y5 0.624 0.358 1.741 0.082 ## .y2 ~~ ## .y4 1.313 0.702 1.871 0.061 ## .y6 2.153 0.734 2.934 0.003 ## .y3 ~~ ## .y7 0.795 0.608 1.308 0.191 ## .y4 ~~ ## .y8 0.348 0.442 0.787 0.431 ## .y6 ~~ ## .y8 1.356 0.568 2.386 0.017 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.082 0.019 4.184 0.000 ## .x2 0.120 0.070 1.718 0.086 ## .x3 0.467 0.090 5.177 0.000 ## .y1 1.891 0.444 4.256 0.000 ## .y2 7.373 1.374 5.366 0.000 ## .y3 5.067 0.952 5.324 0.000 ## .y4 3.148 0.739 4.261 0.000 ## .y5 2.351 0.480 4.895 0.000 ## .y6 4.954 0.914 5.419 0.000 ## .y7 3.431 0.713 4.814 0.000 ## .y8 3.254 0.695 4.685 0.000 ## ind60 0.448 0.087 5.173 0.000 ## .dem60 3.956 0.921 4.295 0.000 ## .dem65 0.172 0.215 0.803 0.422 12.3 Additional tricks 12.3.1 Missing data Often, we have missing data. Let’s simulate what that might be like. library(tidyverse) set.seed(314159) sem_df2 &lt;- sem_df %&gt;% mutate(x1 = ifelse( test = x2/max(x2) &lt; runif(n()), yes = NA, no = x1)) # What proportion of cases are now missing? mean(is.na(sem_df2$x1)) ## [1] 0.4133333 Now we’ll re_run our model, but with our new dataframe that includes missing data. fit &lt;- sem(full_model, data = sem_df2) summary(fit) ## lavaan 0.6-3 ended normally after 62 iterations ## ## Optimization method NLMINB ## Number of free parameters 31 ## ## Used Total ## Number of observations 44 75 ## ## Estimator ML ## Model Fit Test Statistic 43.176 ## Degrees of freedom 35 ## P-value (Chi-square) 0.161 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## ind60 =~ ## x1 1.000 ## x2 1.833 0.182 10.054 0.000 ## x3 1.712 0.208 8.238 0.000 ## dem60 =~ ## y1 1.000 ## y2 1.085 0.191 5.681 0.000 ## y3 0.929 0.158 5.889 0.000 ## y4 1.272 0.153 8.315 0.000 ## dem65 =~ ## y5 1.000 ## y6 1.178 0.179 6.582 0.000 ## y7 1.136 0.148 7.666 0.000 ## y8 1.243 0.173 7.178 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dem60 ~ ## ind60 2.270 0.556 4.080 0.000 ## dem65 ~ ## ind60 -0.262 0.353 -0.743 0.457 ## dem60 0.979 0.131 7.480 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1 ~~ ## .y5 0.088 0.338 0.262 0.793 ## .y2 ~~ ## .y4 0.803 0.780 1.030 0.303 ## .y6 2.365 0.902 2.622 0.009 ## .y3 ~~ ## .y7 1.430 0.654 2.187 0.029 ## .y4 ~~ ## .y8 0.376 0.579 0.649 0.516 ## .y6 ~~ ## .y8 1.430 0.701 2.041 0.041 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.091 0.029 3.183 0.001 ## .x2 0.135 0.075 1.789 0.074 ## .x3 0.414 0.109 3.810 0.000 ## .y1 1.099 0.404 2.719 0.007 ## .y2 7.071 1.614 4.380 0.000 ## .y3 4.752 1.097 4.330 0.000 ## .y4 3.282 0.896 3.662 0.000 ## .y5 1.611 0.493 3.270 0.001 ## .y6 4.392 1.042 4.215 0.000 ## .y7 2.346 0.647 3.625 0.000 ## .y8 3.562 0.935 3.810 0.000 ## ind60 0.393 0.104 3.795 0.000 ## .dem60 3.765 1.039 3.623 0.000 ## .dem65 0.225 0.285 0.787 0.431 Wow! Our results are really different. Our systematically missing data have really biased the results. To fix that, we need Full Information Maximum Likelihood estimation, which we can achieve by setting missing = 'ML' in our sem() call. fit &lt;- sem(full_model, data = sem_df2, missing = &#39;ML&#39;) summary(fit) ## lavaan 0.6-3 ended normally after 78 iterations ## ## Optimization method NLMINB ## Number of free parameters 42 ## ## Number of observations 75 ## Number of missing patterns 2 ## ## Estimator ML ## Model Fit Test Statistic 33.361 ## Degrees of freedom 35 ## P-value (Chi-square) 0.547 ## ## Parameter Estimates: ## ## Information Observed ## Observed information based on Hessian ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## ind60 =~ ## x1 1.000 ## x2 1.938 0.175 11.059 0.000 ## x3 1.606 0.174 9.211 0.000 ## dem60 =~ ## y1 1.000 ## y2 1.256 0.186 6.769 0.000 ## y3 1.058 0.148 7.133 0.000 ## y4 1.265 0.151 8.385 0.000 ## dem65 =~ ## y5 1.000 ## y6 1.187 0.172 6.911 0.000 ## y7 1.283 0.161 7.986 0.000 ## y8 1.268 0.164 7.753 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dem60 ~ ## ind60 1.307 0.364 3.591 0.000 ## dem65 ~ ## ind60 0.505 0.209 2.418 0.016 ## dem60 0.837 0.099 8.464 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1 ~~ ## .y5 0.631 0.369 1.711 0.087 ## .y2 ~~ ## .y4 1.317 0.700 1.881 0.060 ## .y6 2.144 0.726 2.954 0.003 ## .y3 ~~ ## .y7 0.792 0.620 1.277 0.202 ## .y4 ~~ ## .y8 0.350 0.457 0.765 0.444 ## .y6 ~~ ## .y8 1.356 0.572 2.371 0.018 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 5.012 0.103 48.594 0.000 ## .x2 4.792 0.173 27.657 0.000 ## .x3 3.558 0.161 22.066 0.000 ## .y1 5.465 0.302 18.106 0.000 ## .y2 4.256 0.450 9.463 0.000 ## .y3 6.563 0.376 17.458 0.000 ## .y4 4.453 0.384 11.598 0.000 ## .y5 5.136 0.301 17.088 0.000 ## .y6 2.978 0.386 7.719 0.000 ## .y7 6.196 0.377 16.427 0.000 ## .y8 4.043 0.371 10.889 0.000 ## ind60 0.000 ## .dem60 0.000 ## .dem65 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.098 0.030 3.217 0.001 ## .x2 0.106 0.081 1.298 0.194 ## .x3 0.476 0.095 5.019 0.000 ## .y1 1.891 0.469 4.034 0.000 ## .y2 7.376 1.346 5.479 0.000 ## .y3 5.066 0.968 5.231 0.000 ## .y4 3.150 0.757 4.162 0.000 ## .y5 2.365 0.490 4.828 0.000 ## .y6 4.952 0.896 5.525 0.000 ## .y7 3.410 0.724 4.707 0.000 ## .y8 3.251 0.706 4.606 0.000 ## ind60 0.571 0.134 4.270 0.000 ## .dem60 3.964 0.948 4.182 0.000 ## .dem65 0.175 0.220 0.795 0.427 Our results are much improved! Although they are not exactly the same as the full data, they are much closer than before. 12.3.2 Robus estimators The lavaan package includes a few robust estimators. To change which one you are using - say to Robust Maximum Likelihood - we just need to change the estimator parameter. fit &lt;- sem(full_model, data = sem_df2, estimator = &#39;MLR&#39;, missing = &#39;ML&#39;) summary(fit) ## lavaan 0.6-3 ended normally after 78 iterations ## ## Optimization method NLMINB ## Number of free parameters 42 ## ## Number of observations 75 ## Number of missing patterns 2 ## ## Estimator ML Robust ## Model Fit Test Statistic 33.361 34.793 ## Degrees of freedom 35 35 ## P-value (Chi-square) 0.547 0.478 ## Scaling correction factor 0.959 ## for the Yuan-Bentler correction (Mplus variant) ## ## Parameter Estimates: ## ## Information Observed ## Observed information based on Hessian ## Standard Errors Robust.huber.white ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## ind60 =~ ## x1 1.000 ## x2 1.938 0.146 13.268 0.000 ## x3 1.606 0.151 10.644 0.000 ## dem60 =~ ## y1 1.000 ## y2 1.256 0.150 8.398 0.000 ## y3 1.058 0.131 8.104 0.000 ## y4 1.265 0.146 8.657 0.000 ## dem65 =~ ## y5 1.000 ## y6 1.187 0.181 6.544 0.000 ## y7 1.283 0.172 7.449 0.000 ## y8 1.268 0.189 6.712 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dem60 ~ ## ind60 1.307 0.317 4.126 0.000 ## dem65 ~ ## ind60 0.505 0.194 2.603 0.009 ## dem60 0.837 0.088 9.549 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1 ~~ ## .y5 0.631 0.453 1.395 0.163 ## .y2 ~~ ## .y4 1.317 0.728 1.808 0.071 ## .y6 2.144 0.862 2.487 0.013 ## .y3 ~~ ## .y7 0.792 0.614 1.290 0.197 ## .y4 ~~ ## .y8 0.350 0.433 0.807 0.420 ## .y6 ~~ ## .y8 1.356 0.737 1.840 0.066 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 5.012 0.099 50.843 0.000 ## .x2 4.792 0.173 27.657 0.000 ## .x3 3.558 0.161 22.066 0.000 ## .y1 5.465 0.301 18.166 0.000 ## .y2 4.256 0.453 9.402 0.000 ## .y3 6.563 0.376 17.441 0.000 ## .y4 4.453 0.384 11.590 0.000 ## .y5 5.136 0.300 17.140 0.000 ## .y6 2.978 0.387 7.698 0.000 ## .y7 6.196 0.377 16.439 0.000 ## .y8 4.043 0.372 10.862 0.000 ## ind60 0.000 ## .dem60 0.000 ## .dem65 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.098 0.025 3.864 0.000 ## .x2 0.106 0.082 1.289 0.197 ## .x3 0.476 0.092 5.154 0.000 ## .y1 1.891 0.476 3.975 0.000 ## .y2 7.376 1.295 5.696 0.000 ## .y3 5.066 1.086 4.664 0.000 ## .y4 3.150 0.795 3.964 0.000 ## .y5 2.365 0.602 3.929 0.000 ## .y6 4.952 0.898 5.515 0.000 ## .y7 3.410 0.617 5.528 0.000 ## .y8 3.251 0.898 3.620 0.000 ## ind60 0.571 0.113 5.066 0.000 ## .dem60 3.964 0.912 4.347 0.000 ## .dem65 0.175 0.225 0.777 0.437 "]
]
